# GraphRAG Infrastructure Docker Compose
# Includes: NebulaGraph, PostgreSQL, OpenSearch, MinIO, Redis, Qdrant, Langfuse
#
# Environment File: .env.graphrag
#   All services read from .env.graphrag via env_file directive.
#   Copy .env.graphrag.example to .env.graphrag and customize as needed.
#
# Usage:
#
#   # Development (app + chat-widget + all infrastructure + Langfuse)
#   docker compose -f docker-compose.graphrag.yml --profile development up -d
#
#   # Production (app + chat-widget + all infrastructure + Langfuse)
#   docker compose -f docker-compose.graphrag.yml --profile production up -d
#
#   # Infrastructure only (no app, no Langfuse)
#   docker compose -f docker-compose.graphrag.yml up -d
#
#   # Infrastructure + Langfuse only
#   docker compose -f docker-compose.graphrag.yml --profile langfuse up -d
#
#   # Chat Widget only (development)
#   docker compose -f docker-compose.graphrag.yml --profile widget-dev up -d chat-widget-dev
#
#   # Chat Widget only (production)
#   docker compose -f docker-compose.graphrag.yml --profile widget up -d chat-widget
#
#   # LiteLLM only (LLM gateway proxy)
#   docker compose -f docker-compose.graphrag.yml --profile litellm up -d litellm
#
#   # Initialize MinIO buckets manually (optional - now auto-runs with langfuse/development/production profiles)
#   docker compose -f docker-compose.graphrag.yml --profile init run --rm minio-init
#
# Langfuse integration notes:
# - Langfuse uses its own PostgreSQL and Redis instances (different migration/policy requirements)
# - Langfuse shares MinIO (separate bucket: langfuse)
# - ClickHouse is Langfuse-specific for analytics
# - Headless initialization (auto-create org/project/user) requires LANGFUSE_INIT_* variables
#
# Port mapping:
#   GraphRAG:    15xxx-19xxx (postgres:15432, redis:16379, qdrant:16333, minio:19000, opensearch:19200, app:18000)
#   Chat Widget: 4202 (dev), 14203 (prod)
#   LiteLLM:     4000 (LLM proxy gateway)
#   Langfuse:    23xxx-29xxx (web:23000->3000, worker:23030->3030, postgres:25432, redis:26379, clickhouse:28123)
#   NebulaGraph: 29669, 39669

services:
  # ============================================================
  # NebulaGraph Cluster (Graph Database) - v3.8.0
  # ============================================================

  nebula-metad:
    image: vesoft/nebula-metad:v3.8.0
    container_name: graphrag-nebula-metad
    environment:
      USER: root
      TZ: Asia/Taipei
    command:
      - --meta_server_addrs=nebula-metad:9559
      - --local_ip=nebula-metad
      - --ws_ip=nebula-metad
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    volumes:
      - nebula-metad-data:/data/meta
      - nebula-metad-logs:/logs
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://nebula-metad:19559/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  nebula-storaged:
    image: vesoft/nebula-storaged:v3.8.0
    container_name: graphrag-nebula-storaged
    environment:
      USER: root
      TZ: Asia/Taipei
    command:
      - --meta_server_addrs=nebula-metad:9559
      - --local_ip=nebula-storaged
      - --ws_ip=nebula-storaged
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    volumes:
      - nebula-storaged-data:/data/storage
      - nebula-storaged-logs:/logs
    networks:
      - graphrag-network
    depends_on:
      - nebula-metad
    healthcheck:
      test: ["CMD-SHELL", "cat < /dev/null > /dev/tcp/localhost/9779 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  nebula-graphd:
    image: vesoft/nebula-graphd:v3.8.0
    container_name: graphrag-nebula-graphd
    environment:
      USER: root
      TZ: Asia/Taipei
    command:
      - --meta_server_addrs=nebula-metad:9559
      - --local_ip=nebula-graphd
      - --ws_ip=nebula-graphd
      - --port=9669
      - --ws_http_port=19669
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
    ports:
      - "29669:9669"   # Graph query service
      - "39669:19669"  # HTTP status
    volumes:
      - nebula-graphd-logs:/logs
    networks:
      - graphrag-network
    depends_on:
      - nebula-storaged
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://nebula-graphd:19669/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  # NebulaGraph Console - Auto-registers storage hosts
  # This service must complete ADD HOSTS before storaged becomes healthy
  nebula-console:
    image: vesoft/nebula-console:v3.8.0
    container_name: graphrag-nebula-console
    entrypoint: ""
    command:
      - sh
      - -c
      - |
        echo "Waiting for NebulaGraph to be ready..."
        sleep 10
        for i in $$(seq 1 60); do
          result=$$(nebula-console -addr nebula-graphd -port 9669 -u root -p nebula -e 'ADD HOSTS "nebula-storaged":9779' 2>&1)
          if [ $$? -eq 0 ]; then
            echo "Storage host registered successfully!"
            touch /tmp/nebula-ready
            break
          fi
          echo "Retry $$i: waiting for graph service..."
          sleep 2
        done
        echo "NebulaGraph Console ready. Connect with: nebula-console -addr nebula-graphd -port 9669 -u root -p nebula"
        tail -f /dev/null
    networks:
      - graphrag-network
    depends_on:
      nebula-graphd:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "test", "-f", "/tmp/nebula-ready"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 15s

  # ============================================================
  # Qdrant Vector Database (Dense + Sparse vectors)
  # ============================================================

  qdrant:
    image: qdrant/qdrant:v1.15.1
    container_name: graphrag-qdrant
    ports:
      - "16333:6333"  # REST API
      - "16334:6334"  # gRPC
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__LOG_LEVEL=INFO
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/6333' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ============================================================
  # OpenSearch 3.x (Full-text Search)
  # Based on official docs: https://docs.opensearch.org/latest/install-and-configure/install-opensearch/docker/
  # Using single-node mode for development
  # Breaking changes: https://docs.opensearch.org/latest/breaking-changes/
  # ============================================================

  opensearch:
    image: opensearchproject/opensearch:3.4.0
    container_name: graphrag-opensearch
    environment:
      - cluster.name=graphrag-cluster
      - node.name=opensearch-node1
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - DISABLE_INSTALL_DEMO_CONFIG=true
      - DISABLE_SECURITY_PLUGIN=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "19200:9200"  # REST API
      - "19600:9600"  # Performance Analyzer
    volumes:
      - opensearch-data:/usr/share/opensearch/data
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # OpenSearch Dashboards (optional, for debugging)
  # Enable with: docker compose -f docker-compose.graphrag.yml --profile debug up -d
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:3.4.0
    container_name: graphrag-opensearch-dashboards
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "15601:5601"
    networks:
      - graphrag-network
    depends_on:
      opensearch:
        condition: service_healthy
    profiles:
      - debug
      - development
    restart: unless-stopped

  # ============================================================
  # PostgreSQL (Metadata Database)
  # ============================================================

  postgres:
    image: postgres:16-alpine
    container_name: graphrag-postgres
    environment:
      - POSTGRES_USER=graphrag
      - POSTGRES_PASSWORD=graphrag_secret
      - POSTGRES_DB=graphrag
      - PGDATA=/var/lib/postgresql/data/pgdata
    ports:
      - "15432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      # Init scripts run on first startup (creates additional databases)
      - ./scripts/init-postgres.sh:/docker-entrypoint-initdb.d/init-postgres.sh:ro
      # Schema migrations managed by Alembic (see app-dev/app-prod command)
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U graphrag -d graphrag"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ============================================================
  # Redis (Job Queue + Cache)
  # ============================================================

  redis:
    image: redis:7-alpine
    container_name: graphrag-redis
    ports:
      - "16379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # ============================================================
  # MinIO (Object Storage)
  # ============================================================

  minio:
    image: minio/minio:RELEASE.2024-01-01T16-36-33Z
    container_name: graphrag-minio
    command: server /data --console-address ":9001"
    ports:
      - "19000:9000"   # API
      - "19001:9001"   # Console
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
    volumes:
      - minio-data:/data
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'cat < /dev/null > /dev/tcp/localhost/9000'"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # MinIO bucket initialization
  minio-init:
    image: minio/mc:latest
    container_name: graphrag-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin123;
      mc mb myminio/documents --ignore-existing;
      mc mb myminio/chunks --ignore-existing;
      mc mb myminio/canonical --ignore-existing;
      mc mb myminio/assets --ignore-existing;
      mc mb myminio/reports --ignore-existing;
      mc mb myminio/langfuse --ignore-existing;
      echo 'Buckets created successfully (including langfuse)';
      exit 0;
      "
    networks:
      - graphrag-network
    profiles:
      - init
      - langfuse
      - development
      - production

  # ============================================================
  # GraphRAG Application (Development)
  # ============================================================

  app-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: graphrag-app-dev
    ports:
      - "18000:8000"
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./rag_test_data:/app/rag_test_data
      - ./huggingface:/root/.cache/huggingface
    env_file:
      - .env.graphrag
    environment:
      # Override for development mode
      - RELOAD=True
      - DEBUG=True
    command: >
      sh -c "
        echo 'Running database migrations...' &&
        alembic -c src/chatbot_graphrag/alembic.ini upgrade head &&
        echo 'Starting application...' &&
        python -m chatbot_graphrag.cli dev
      "
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      minio:
        condition: service_healthy
      nebula-console:
        condition: service_healthy
    networks:
      - graphrag-network
    profiles:
      - development

  # ============================================================
  # GraphRAG Application (Production)
  # ============================================================

  app-prod:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: graphrag-app-prod
    ports:
      - "18000:8000"
    volumes:
      - ./logs:/app/logs
    env_file:
      - .env.graphrag
    environment:
      # Override for production mode
      - RELOAD=False
      - DEBUG=False
      - WORKERS=8
    command: >
      sh -c "
        echo 'Running database migrations...' &&
        alembic -c src/chatbot_graphrag/alembic.ini upgrade head &&
        echo 'Starting application...' &&
        uvicorn chatbot_graphrag.main:app --host 0.0.0.0 --port 8000 --workers 8
      "
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      minio:
        condition: service_healthy
      nebula-console:
        condition: service_healthy
    networks:
      - graphrag-network
    profiles:
      - production

  # ============================================================
  # Chat Widget Frontend
  # ============================================================

  # Chat Widget 開發模式（熱更新）
  chat-widget-dev:
    build:
      context: ./frontend/chat-widget
      dockerfile: Dockerfile.dev
      target: development
    container_name: graphrag-chat-widget-dev
    ports:
      - "4202:5180"
    volumes:
      - ./frontend/chat-widget/src:/app/src
      - ./frontend/chat-widget/public:/app/public
      - ./frontend/chat-widget/loader:/app/loader
      - ./frontend/chat-widget/index.html:/app/index.html
      - ./frontend/chat-widget/vite.config.js:/app/vite.config.js
      - ./frontend/chat-widget/tailwind.config.js:/app/tailwind.config.js
      - ./frontend/chat-widget/postcss.config.js:/app/postcss.config.js
      - ./frontend/chat-widget/svelte.config.js:/app/svelte.config.js
    environment:
      - API_TARGET=http://app-dev:8000
    depends_on:
      - app-dev
    networks:
      - graphrag-network
    profiles:
      - development
      - widget-dev

  # Chat Widget 生產模式（Nginx 靜態伺服）
  chat-widget:
    build:
      context: ./frontend/chat-widget
      dockerfile: Dockerfile
      target: production
    container_name: graphrag-chat-widget
    ports:
      - "14203:80"
    environment:
      - API_TARGET=http://app-prod:8000
    depends_on:
      - app-prod
    networks:
      - graphrag-network
    profiles:
      - production
      - widget

  # ============================================================
  # LiteLLM Proxy (LLM Gateway)
  # ============================================================
  # LiteLLM provides a unified OpenAI-compatible API for 100+ LLM providers
  # Documentation: https://docs.litellm.ai/docs/
  #
  # Setup:
  #   1. Create config/litellm_config.yaml with your model definitions
  #   2. Set LITELLM_MASTER_KEY in .env.graphrag (must start with sk-)
  #   3. Run with: docker compose -f docker-compose.graphrag.yml --profile litellm up -d
  #
  # Usage:
  #   API endpoint: http://localhost:14000
  #   Health check: http://localhost:14000/health
  #   Admin UI: http://localhost:14000/ui (requires LITELLM_MASTER_KEY)
  # ============================================================

  litellm:
    image: ghcr.io/berriai/litellm:v1.80.11.rc.1
    container_name: graphrag-litellm
    ports:
      - "4000:4000"   # LiteLLM API (OpenAI-compatible)
    volumes:
      - ./config/litellm_config.yaml:/app/config.yaml:ro
    environment:
      # Master key for admin access (REQUIRED - must start with sk-)
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-litellm-master-key-change-me}
      # Salt key for encrypting API credentials in DB (REQUIRED if using DB)
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY:-litellm-salt-key-change-me}
      # Optional: Use PostgreSQL for persistent config (shares with GraphRAG)
      - DATABASE_URL=postgresql://graphrag:graphrag_secret@postgres:5432/litellm
      # Enable storing models in database (required for UI/API model management)
      - STORE_MODEL_IN_DB=True
      # Proxy settings
      - LITELLM_LOG=DEBUG
      # Pass through API keys from environment
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - AZURE_API_KEY=${AZURE_API_KEY:-}
      - AZURE_API_BASE=${AZURE_API_BASE:-}
      - AZURE_API_VERSION=${AZURE_API_VERSION:-}
    command: ["--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0"]
    networks:
      - graphrag-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:4000/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    profiles:
      - litellm
      - development
      - production

  # ============================================================
  # Background Workers
  # ============================================================

  ingest-worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: graphrag-ingest-worker
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    env_file:
      - .env.graphrag
    command: python -m chatbot_graphrag.workers.ingest_worker
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      nebula-console:
        condition: service_healthy
    networks:
      - graphrag-network
    profiles:
      - workers

  community-worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: graphrag-community-worker
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    env_file:
      - .env.graphrag
    command: python -m chatbot_graphrag.workers.community_worker
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      nebula-console:
        condition: service_healthy
    networks:
      - graphrag-network
    profiles:
      - workers

  # ============================================================
  # Langfuse Observability Platform
  # ============================================================
  # Langfuse requires its own PostgreSQL and Redis due to:
  # - PostgreSQL: Langfuse runs its own migrations at startup
  # - Redis: Requires noeviction policy (GraphRAG uses allkeys-lru)
  # MinIO is shared with a separate bucket (langfuse)
  # ============================================================

  # Langfuse PostgreSQL (separate from GraphRAG)
  langfuse-postgres:
    image: postgres:17-alpine
    container_name: graphrag-langfuse-postgres
    environment:
      - POSTGRES_USER=langfuse
      - POSTGRES_PASSWORD=langfuse_secret
      - POSTGRES_DB=langfuse
      - PGDATA=/var/lib/postgresql/data/pgdata
      - TZ=UTC
      - PGTZ=UTC
    ports:
      - "25432:5432"
    volumes:
      - langfuse-postgres-data:/var/lib/postgresql/data
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langfuse -d langfuse"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s
    restart: unless-stopped
    profiles:
      - langfuse
      - development
      - production

  # Langfuse Redis (separate - requires noeviction policy)
  langfuse-redis:
    image: redis:7-alpine
    container_name: graphrag-langfuse-redis
    ports:
      - "26379:6379"
    volumes:
      - langfuse-redis-data:/data
    # Langfuse requires noeviction (cannot lose queue data)
    command: >
      --requirepass langfuse_redis_secret
      --maxmemory 512mb
      --maxmemory-policy noeviction
      --appendonly yes
    networks:
      - graphrag-network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "langfuse_redis_secret", "ping"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 5s
    restart: unless-stopped
    profiles:
      - langfuse
      - development
      - production

  # ClickHouse (Langfuse analytics database)
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: graphrag-clickhouse
    user: "101:101"
    environment:
      - CLICKHOUSE_DB=langfuse
      - CLICKHOUSE_USER=langfuse
      - CLICKHOUSE_PASSWORD=clickhouse_secret
    ports:
      - "28123:8123"  # HTTP API
      - "29000:9000"  # Native TCP (for migrations)
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
    networks:
      - graphrag-network
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s
    restart: unless-stopped
    profiles:
      - langfuse
      - development
      - production

  # Langfuse Worker (background processing)
  langfuse-worker:
    image: langfuse/langfuse-worker:3
    container_name: graphrag-langfuse-worker
    restart: always
    env_file:
      - .env.graphrag
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
      langfuse-redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - "23030:3030"
    environment: &langfuse-env
      # Database
      DATABASE_URL: postgresql://langfuse:langfuse_secret@langfuse-postgres:5432/langfuse
      # Auth & Security (CHANGE THESE IN PRODUCTION!)
      SALT: ${LANGFUSE_SALT:-langfuse_salt_change_me_in_production}
      ENCRYPTION_KEY: ${LANGFUSE_ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000}
      # ClickHouse
      CLICKHOUSE_MIGRATION_URL: clickhouse://clickhouse:9000
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: langfuse
      CLICKHOUSE_PASSWORD: clickhouse_secret
      CLICKHOUSE_CLUSTER_ENABLED: "false"
      # Redis
      REDIS_HOST: langfuse-redis
      REDIS_PORT: "6379"
      REDIS_AUTH: langfuse_redis_secret
      # MinIO (shared with GraphRAG)
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: langfuse
      LANGFUSE_S3_EVENT_UPLOAD_REGION: auto
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: minioadmin
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: minioadmin123
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: http://minio:9000
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: events/
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: langfuse
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: auto
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: minioadmin
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: minioadmin123
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: http://minio:9000
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: media/
      # Features
      TELEMETRY_ENABLED: ${LANGFUSE_TELEMETRY_ENABLED:-false}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"
    networks:
      - graphrag-network
    profiles:
      - langfuse
      - development
      - production

  # Langfuse Web (main UI)
  langfuse-web:
    image: langfuse/langfuse:3
    container_name: graphrag-langfuse-web
    restart: always
    env_file:
      - .env.graphrag
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
      langfuse-redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      langfuse-worker:
        condition: service_started
    ports:
      - "23000:3000"  # Langfuse default port
    environment:
      <<: *langfuse-env
      PORT: "3000"  # Override env_file's PORT=8000 (for GraphRAG app)
      NEXTAUTH_URL: http://localhost:23000
      NEXTAUTH_SECRET: langfuse_nextauth_secret_change_me
      # Headless Initialization variables are read from env_file (.env.graphrag)
      # Required: LANGFUSE_INIT_ORG_ID, LANGFUSE_INIT_PROJECT_ID,
      #           LANGFUSE_INIT_PROJECT_PUBLIC_KEY, LANGFUSE_INIT_PROJECT_SECRET_KEY,
      #           LANGFUSE_INIT_USER_EMAIL, LANGFUSE_INIT_USER_PASSWORD
    networks:
      - graphrag-network
    profiles:
      - langfuse
      - development
      - production

networks:
  graphrag-network:
    driver: bridge
    name: graphrag-network

volumes:
  # NebulaGraph volumes
  nebula-metad-data:
    driver: local
  nebula-metad-logs:
    driver: local
  nebula-storaged-data:
    driver: local
  nebula-storaged-logs:
    driver: local
  nebula-graphd-logs:
    driver: local
  # Other services
  qdrant-data:
    driver: local
  opensearch-data:
    driver: local
  postgres-data:
    driver: local
  redis-data:
    driver: local
  minio-data:
    driver: local
  # Langfuse volumes
  langfuse-postgres-data:
    driver: local
  langfuse-redis-data:
    driver: local
  clickhouse-data:
    driver: local
  clickhouse-logs:
    driver: local
