"""
Langfuse Prompt Management æœå‹™å°è£ã€‚

æ­¤æ¨¡çµ„æä¾›çµ±ä¸€çš„ Langfuse Prompt ç®¡ç†åŠŸèƒ½ï¼š
1. å¿«å–æ©Ÿåˆ¶ï¼šæ¸›å°‘ API å‘¼å«ï¼Œé è¨­ 5 åˆ†é˜ TTL
2. åƒ…å¿«å–ç„¡ Fallbackï¼šå®Œå…¨ä¾è³´ Langfuseï¼Œå¼·åˆ¶è¦æ±‚æœå‹™å¯ç”¨
3. åŸºæ–¼ Label çš„ A/B Testingï¼šé€é default_label åƒæ•¸åˆ‡æ› production/staging
4. Trace Linkingï¼šè¿”å› metadata ä¾›è¿½è¹¤ prompt ç‰ˆæœ¬
5. Hash-based ç‰ˆæœ¬è¿½è¹¤ï¼šå…§å®¹ hash ç”¨æ–¼åµæ¸¬ prompt è®Šæ›´
6. ç·¨è­¯çµæœå¿«å–ï¼šæ¸›å°‘é‡è¤‡ç·¨è­¯çš„é–‹éŠ·
"""

import hashlib
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Tuple

from langfuse import get_client
from loguru import logger


def _compute_content_hash(content: Any) -> str:
    """
    è¨ˆç®— prompt å…§å®¹çš„ hash å€¼ã€‚

    Args:
        content: Prompt å…§å®¹ï¼ˆå­—ä¸²æˆ–çµæ§‹åŒ–è³‡æ–™ï¼‰

    Returns:
        å…§å®¹çš„ MD5 hashï¼ˆå‰ 16 å­—å…ƒï¼‰
    """
    if isinstance(content, str):
        data = content.encode("utf-8")
    else:
        # å°æ–¼è¤‡é›œçµæ§‹ï¼Œåºåˆ—åŒ–å¾Œè¨ˆç®— hash
        import json
        data = json.dumps(content, sort_keys=True, ensure_ascii=False).encode("utf-8")
    return hashlib.md5(data).hexdigest()[:16]


@dataclass
class CachedPrompt:
    """å¿«å–çš„ Prompt è³‡æ–™"""

    prompt: Any  # Langfuse Prompt object
    cached_at: datetime
    version: int
    label: str
    content_hash: str = ""  # Prompt å…§å®¹çš„ hashï¼Œç”¨æ–¼åµæ¸¬è®Šæ›´


@dataclass
class PromptMetadata:
    """Prompt å…ƒè³‡æ–™ï¼Œä¾› trace linking ä½¿ç”¨"""

    name: str
    version: int
    label: str
    langfuse_prompt: Any  # åŸå§‹ Langfuse prompt object


@dataclass
class PromptVersionInfo:
    """ç”¨æ–¼ Telemetry è¨˜éŒ„çš„ Prompt ç‰ˆæœ¬è³‡è¨Š"""

    name: str
    version: int
    label: str

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "version": self.version,
            "label": self.label,
        }


@dataclass
class CompiledPromptCache:
    """ç·¨è­¯å¾Œçš„ Prompt å¿«å–"""

    compiled: str
    cached_at: datetime
    source_hash: str  # ä¾†æº prompt çš„ hash


class PromptService:
    """
    Langfuse Prompt Management æœå‹™å°è£

    è¨­è¨ˆæ±ºç­–ï¼š
    - åƒ…å¿«å–ç„¡ Fallbackï¼šå®Œå…¨ä¾è³´ Langfuseï¼Œå¿«å–éæœŸä¸” API å¤±æ•—æ™‚æ‹‹å‡ºç•°å¸¸
    - åŸºæ–¼ Label çš„ A/B Testingï¼šé€é default_label åƒæ•¸åˆ‡æ› production/staging
    - Hash-based ç‰ˆæœ¬è¿½è¹¤ï¼šä½¿ç”¨å…§å®¹ hash åµæ¸¬ prompt è®Šæ›´
    - ç·¨è­¯çµæœå¿«å–ï¼šå°æ–¼ç›¸åŒè®Šæ•¸çµ„åˆçš„ç·¨è­¯çµæœé€²è¡Œå¿«å–

    ä½¿ç”¨ç¯„ä¾‹ï¼š
        prompt_service = PromptService(default_label="production")

        # ç²å–ä¸¦ç·¨è­¯ prompt
        content, metadata = prompt_service.get_text_prompt(
            "unified-agent-system",
            language_instruction=lang_inst,
            support_scope=scope_text,
        )

        # åœ¨ telemetry ä¸­è¨˜éŒ„ç‰ˆæœ¬
        prompt_service.record_prompt_usage("unified-agent-system", metadata)
    """

    def __init__(
        self,
        default_label: str = "production",
        cache_ttl_seconds: int = 300,
        compiled_cache_ttl_seconds: int = 600,
    ):
        """
        åˆå§‹åŒ– PromptServiceã€‚

        Args:
            default_label: é è¨­ prompt labelï¼Œå¯é¸ "production" æˆ– "staging"
            cache_ttl_seconds: Prompt å¿«å–å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 300 ç§’ï¼ˆ5 åˆ†é˜ï¼‰
            compiled_cache_ttl_seconds: ç·¨è­¯çµæœå¿«å–æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 600 ç§’ï¼ˆ10 åˆ†é˜ï¼‰
        """
        self.langfuse = get_client()
        self.default_label = default_label
        self.cache_ttl = timedelta(seconds=cache_ttl_seconds)
        self.compiled_cache_ttl = timedelta(seconds=compiled_cache_ttl_seconds)
        self._cache: Dict[str, CachedPrompt] = {}
        self._compiled_cache: Dict[str, CompiledPromptCache] = {}
        self._used_prompts: Dict[str, PromptVersionInfo] = {}
        self._cache_stats: Dict[str, int] = {"hits": 0, "misses": 0, "compiled_hits": 0}

    def get_prompt(
        self,
        name: str,
        *,
        label: Optional[str] = None,
        version: Optional[int] = None,
        prompt_type: Optional[str] = None,
    ) -> tuple[Any, PromptMetadata]:
        """
        ç²å– Promptï¼Œå„ªå…ˆä½¿ç”¨å¿«å–ã€‚

        Args:
            name: Prompt åç¨±
            label: Prompt labelï¼ŒæœªæŒ‡å®šæ™‚ä½¿ç”¨ default_label
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿï¼ŒæœªæŒ‡å®šæ™‚ä½¿ç”¨ label å°æ‡‰çš„ç‰ˆæœ¬
            prompt_type: Prompt é¡å‹ ("text" æˆ– "chat")ï¼ŒæœªæŒ‡å®šæ™‚ç”± Langfuse è‡ªå‹•åˆ¤æ–·

        Returns:
            tuple[Any, PromptMetadata]: (Langfuse Prompt object, å…ƒè³‡æ–™)

        Raises:
            Exception: ç•¶å¿«å–éæœŸä¸” Langfuse API ä¸å¯ç”¨æ™‚
        """
        effective_label = label or self.default_label
        cache_key = f"{name}:{effective_label}:{version or 'latest'}"

        # æª¢æŸ¥å¿«å–
        if cache_key in self._cache:
            cached = self._cache[cache_key]
            if datetime.now() - cached.cached_at < self.cache_ttl:
                self._cache_stats["hits"] += 1
                logger.debug(
                    f"[PromptService] Cache hit: {cache_key} (hash={cached.content_hash})"
                )
                metadata = PromptMetadata(
                    name=name,
                    version=cached.version,
                    label=cached.label,
                    langfuse_prompt=cached.prompt,
                )
                self._record_usage(metadata)
                return cached.prompt, metadata

        self._cache_stats["misses"] += 1

        # å¾ Langfuse ç²å–
        logger.info(
            f"[PromptService] Fetching prompt: {name}, label={effective_label}"
        )

        # æ§‹å»º get_prompt åƒæ•¸
        get_prompt_kwargs: Dict[str, Any] = {}
        if version is not None:
            get_prompt_kwargs["version"] = version
        else:
            get_prompt_kwargs["label"] = effective_label
        if prompt_type is not None:
            get_prompt_kwargs["type"] = prompt_type

        prompt = self.langfuse.get_prompt(name, **get_prompt_kwargs)

        # è¨ˆç®—å…§å®¹ hash
        prompt_content = getattr(prompt, "prompt", None) or ""
        content_hash = _compute_content_hash(prompt_content)

        # æª¢æŸ¥æ˜¯å¦æœ‰èˆŠå¿«å–ä¸”å…§å®¹ç›¸åŒï¼ˆç‰ˆæœ¬æ›´æ–°ä½†å…§å®¹æœªè®Šï¼‰
        if cache_key in self._cache:
            old_cached = self._cache[cache_key]
            if old_cached.content_hash == content_hash:
                logger.debug(
                    f"[PromptService] Content unchanged, extending cache: {cache_key}"
                )

        # æ›´æ–°å¿«å–
        self._cache[cache_key] = CachedPrompt(
            prompt=prompt,
            cached_at=datetime.now(),
            version=prompt.version,
            label=effective_label,
            content_hash=content_hash,
        )

        metadata = PromptMetadata(
            name=name,
            version=prompt.version,
            label=effective_label,
            langfuse_prompt=prompt,
        )
        self._record_usage(metadata)
        return prompt, metadata

    def get_text_prompt(
        self,
        name: str,
        *,
        label: Optional[str] = None,
        version: Optional[int] = None,
        use_compiled_cache: bool = True,
        **compile_vars,
    ) -> tuple[str, PromptMetadata]:
        """
        ç²å– Text Prompt ä¸¦ç·¨è­¯è®Šæ•¸ã€‚

        Args:
            name: Prompt åç¨±
            label: Prompt label
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿ
            use_compiled_cache: æ˜¯å¦ä½¿ç”¨ç·¨è­¯çµæœå¿«å–ï¼ˆé è¨­ Trueï¼‰
            **compile_vars: ç·¨è­¯æ™‚æ›¿æ›çš„è®Šæ•¸

        Returns:
            tuple[str, PromptMetadata]: (ç·¨è­¯å¾Œçš„å­—ä¸², å…ƒè³‡æ–™)
        """
        prompt, metadata = self.get_prompt(
            name, label=label, version=version, prompt_type="text"
        )

        # æª¢æŸ¥ç·¨è­¯çµæœå¿«å–
        if use_compiled_cache and compile_vars:
            # è¨ˆç®—ç·¨è­¯å¿«å–çš„ keyï¼ˆåŒ…å«è®Šæ•¸çš„ hashï¼‰
            vars_hash = _compute_content_hash(compile_vars)
            cache_key = self._cache.get(f"{name}:{label or self.default_label}:{version or 'latest'}")
            source_hash = cache_key.content_hash if cache_key else ""
            compiled_cache_key = f"{name}:{source_hash}:{vars_hash}"

            if compiled_cache_key in self._compiled_cache:
                cached = self._compiled_cache[compiled_cache_key]
                if (
                    datetime.now() - cached.cached_at < self.compiled_cache_ttl
                    and cached.source_hash == source_hash
                ):
                    self._cache_stats["compiled_hits"] += 1
                    logger.debug(
                        f"[PromptService] Compiled cache hit: {name} (vars_hash={vars_hash[:8]})"
                    )
                    return cached.compiled, metadata

        # ç·¨è­¯ prompt
        compiled = prompt.compile(**compile_vars)

        # æ›´æ–°ç·¨è­¯å¿«å–
        if use_compiled_cache and compile_vars:
            cache_entry = self._cache.get(f"{name}:{label or self.default_label}:{version or 'latest'}")
            source_hash = cache_entry.content_hash if cache_entry else ""
            vars_hash = _compute_content_hash(compile_vars)
            compiled_cache_key = f"{name}:{source_hash}:{vars_hash}"
            self._compiled_cache[compiled_cache_key] = CompiledPromptCache(
                compiled=compiled,
                cached_at=datetime.now(),
                source_hash=source_hash,
            )

        return compiled, metadata

    def get_langchain_prompt(
        self,
        name: str,
        *,
        label: Optional[str] = None,
        version: Optional[int] = None,
        **precompile_vars,
    ) -> tuple[str, PromptMetadata]:
        """
        ç²å– Prompt ä¸¦è½‰æ›ç‚º LangChain æ ¼å¼ã€‚

        Langfuse ä½¿ç”¨ {{var}}ï¼ŒLangChain ä½¿ç”¨ {var}ã€‚
        å¯é å…ˆç·¨è­¯éƒ¨åˆ†è®Šæ•¸ï¼Œå…¶é¤˜ç•™çµ¦ LangChain PromptTemplateã€‚

        Args:
            name: Prompt åç¨±
            label: Prompt label
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿ
            **precompile_vars: é å…ˆç·¨è­¯çš„è®Šæ•¸

        Returns:
            tuple[str, PromptMetadata]: (LangChain æ ¼å¼çš„ prompt å­—ä¸², å…ƒè³‡æ–™)
        """
        prompt, metadata = self.get_prompt(
            name, label=label, version=version, prompt_type="text"
        )
        langchain_template = prompt.get_langchain_prompt(**precompile_vars)
        return langchain_template, metadata

    def get_chat_messages(
        self,
        name: str,
        *,
        label: Optional[str] = None,
        version: Optional[int] = None,
        **compile_vars,
    ) -> tuple[list[dict], PromptMetadata]:
        """
        ç²å– Chat Prompt ä¸¦ç·¨è­¯ç‚ºè¨Šæ¯åˆ—è¡¨ã€‚

        Args:
            name: Prompt åç¨±
            label: Prompt label
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿ
            **compile_vars: ç·¨è­¯æ™‚æ›¿æ›çš„è®Šæ•¸

        Returns:
            tuple[list[dict], PromptMetadata]: (è¨Šæ¯åˆ—è¡¨, å…ƒè³‡æ–™)
        """
        prompt, metadata = self.get_prompt(
            name, label=label, version=version, prompt_type="chat"
        )
        messages = prompt.compile(**compile_vars)
        return messages, metadata

    def _record_usage(self, metadata: PromptMetadata) -> None:
        """è¨˜éŒ„ prompt ä½¿ç”¨æƒ…æ³ï¼Œä¾› telemetry ä½¿ç”¨"""
        self._used_prompts[metadata.name] = PromptVersionInfo(
            name=metadata.name,
            version=metadata.version,
            label=metadata.label,
        )

    def get_used_prompts(self) -> Dict[str, Dict[str, Any]]:
        """
        ç²å–æœ¬æ¬¡è«‹æ±‚ä¸­ä½¿ç”¨çš„æ‰€æœ‰ prompt ç‰ˆæœ¬è³‡è¨Šã€‚

        Returns:
            Dict[str, Dict[str, Any]]: prompt åç¨±åˆ°ç‰ˆæœ¬è³‡è¨Šçš„æ˜ å°„
        """
        return {name: info.to_dict() for name, info in self._used_prompts.items()}

    def clear_used_prompts(self) -> None:
        """æ¸…é™¤ä½¿ç”¨è¨˜éŒ„ï¼ˆæ¯æ¬¡è«‹æ±‚çµæŸå¾Œå‘¼å«ï¼‰"""
        self._used_prompts.clear()

    def clear_cache(self, name: Optional[str] = None) -> None:
        """
        æ¸…é™¤å¿«å–ã€‚

        Args:
            name: æŒ‡å®šè¦æ¸…é™¤çš„ prompt åç¨±ï¼ŒæœªæŒ‡å®šå‰‡æ¸…é™¤å…¨éƒ¨
        """
        if name:
            # æ¸…é™¤ prompt å¿«å–
            keys_to_remove = [k for k in self._cache if k.startswith(f"{name}:")]
            for key in keys_to_remove:
                del self._cache[key]
            # æ¸…é™¤ç·¨è­¯å¿«å–
            compiled_keys_to_remove = [
                k for k in self._compiled_cache if k.startswith(f"{name}:")
            ]
            for key in compiled_keys_to_remove:
                del self._compiled_cache[key]
            logger.info(
                f"[PromptService] Cleared cache for: {name} "
                f"(prompts={len(keys_to_remove)}, compiled={len(compiled_keys_to_remove)})"
            )
        else:
            self._cache.clear()
            self._compiled_cache.clear()
            logger.info("[PromptService] Cleared all cache")

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        ç²å–å¿«å–çµ±è¨ˆè³‡è¨Šã€‚

        Returns:
            Dict åŒ…å«ï¼šhits, misses, compiled_hits, hit_rate, prompt_count, compiled_count
        """
        total = self._cache_stats["hits"] + self._cache_stats["misses"]
        hit_rate = self._cache_stats["hits"] / total if total > 0 else 0.0
        return {
            **self._cache_stats,
            "hit_rate": round(hit_rate, 3),
            "prompt_count": len(self._cache),
            "compiled_count": len(self._compiled_cache),
        }

    def reset_cache_stats(self) -> None:
        """é‡ç½®å¿«å–çµ±è¨ˆè¨ˆæ•¸å™¨"""
        self._cache_stats = {"hits": 0, "misses": 0, "compiled_hits": 0}

    def preload_prompts(self, prompt_names: list[str]) -> None:
        """
        é è¼‰ promptsï¼Œç”¨æ–¼æ‡‰ç”¨å•Ÿå‹•æ™‚æ¸›å°‘é¦–æ¬¡è«‹æ±‚å»¶é²ã€‚

        Args:
            prompt_names: è¦é è¼‰çš„ prompt åç¨±åˆ—è¡¨
        """
        for name in prompt_names:
            try:
                self.get_prompt(name)
                logger.info(f"[PromptService] Preloaded: {name}")
            except Exception as exc:
                logger.warning(f"[PromptService] Failed to preload {name}: {exc}")


class DomainAwarePromptService(PromptService):
    """
    é ˜åŸŸæ„ŸçŸ¥çš„ Prompt æœå‹™ã€‚

    æ“´å±• PromptService ä»¥æ”¯æ´ï¼š
    - Domain namespace å‰ç¶´
    - é ˜åŸŸå°ˆå±¬ prompts çš„è‡ªå‹•è¼‰å…¥
    - Fallback åˆ°é€šç”¨ prompts

    ä½¿ç”¨ç¯„ä¾‹ï¼š
        from chatbot_rag.core.domain import get_current_domain

        domain_config = get_current_domain()
        prompt_service = DomainAwarePromptService(
            domain_config=domain_config,
            default_label="production",
        )

        # è‡ªå‹•æ·»åŠ  domain namespace
        content, metadata = prompt_service.get_domain_prompt(
            "intent-analyzer-system",
            language_instruction=lang_inst,
        )
    """

    def __init__(
        self,
        domain_config: Any,
        default_label: str = "production",
        cache_ttl_seconds: int = 300,
        compiled_cache_ttl_seconds: int = 600,
    ):
        """
        åˆå§‹åŒ– DomainAwarePromptServiceã€‚

        Args:
            domain_config: DomainConfig å¯¦ä¾‹
            default_label: é è¨­ prompt label
            cache_ttl_seconds: Prompt å¿«å–å­˜æ´»æ™‚é–“
            compiled_cache_ttl_seconds: ç·¨è­¯çµæœå¿«å–æ™‚é–“
        """
        super().__init__(
            default_label=default_label,
            cache_ttl_seconds=cache_ttl_seconds,
            compiled_cache_ttl_seconds=compiled_cache_ttl_seconds,
        )
        self.domain_config = domain_config
        self._domain_prompts_cache: Dict[str, str] = {}

    def get_domain_prompt(
        self,
        base_name: str,
        *,
        label: Optional[str] = None,
        version: Optional[int] = None,
        use_compiled_cache: bool = True,
        **compile_vars,
    ) -> Tuple[str, Optional[PromptMetadata]]:
        """
        ç²å–é ˜åŸŸå°ˆå±¬çš„ Promptã€‚

        æŸ¥æ‰¾é †åºï¼š
        1. Langfuseï¼ˆå¸¶ domain namespaceï¼‰
        2. é ˜åŸŸå°ˆå±¬ prompts æ¨¡çµ„
        3. Langfuseï¼ˆä¸å¸¶ namespaceï¼‰
        4. é è¨­ prompts

        Args:
            base_name: åŸºç¤ prompt åç¨±ï¼ˆä¸å« namespaceï¼‰
            label: Prompt label
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿ
            use_compiled_cache: æ˜¯å¦ä½¿ç”¨ç·¨è­¯çµæœå¿«å–
            **compile_vars: ç·¨è­¯æ™‚æ›¿æ›çš„è®Šæ•¸

        Returns:
            tuple[str, Optional[PromptMetadata]]: (ç·¨è­¯å¾Œçš„å­—ä¸², å…ƒè³‡æ–™æˆ– None)
        """
        # 1. å˜—è©¦å¾ Langfuse ç²å–ï¼ˆå¸¶ domain namespaceï¼‰
        full_name = self.domain_config.get_prompt_name(base_name)
        try:
            content, metadata = self.get_text_prompt(
                full_name,
                label=label,
                version=version,
                use_compiled_cache=use_compiled_cache,
                **compile_vars,
            )
            return content, metadata
        except Exception as exc:
            logger.debug(
                f"[DomainAwarePromptService] Failed to get {full_name} from Langfuse: {exc}"
            )

        # 2. å˜—è©¦å¾é ˜åŸŸå°ˆå±¬ prompts æ¨¡çµ„ç²å–
        domain_prompts = self.domain_config.get_domain_prompts()
        if base_name in domain_prompts:
            prompt_config = domain_prompts[base_name]
            prompt_content = prompt_config.get("prompt", "")
            if compile_vars:
                # ç°¡å–®çš„è®Šæ•¸æ›¿æ›ï¼ˆMustache é¢¨æ ¼ {{var}}ï¼‰
                for key, value in compile_vars.items():
                    prompt_content = prompt_content.replace(f"{{{{{key}}}}}", str(value))
            logger.debug(
                f"[DomainAwarePromptService] Using domain prompt: {base_name}"
            )
            return prompt_content, None

        # 3. å˜—è©¦å¾ Langfuse ç²å–ï¼ˆä¸å¸¶ namespaceï¼‰
        if self.domain_config.prompt_namespace:
            try:
                content, metadata = self.get_text_prompt(
                    base_name,
                    label=label,
                    version=version,
                    use_compiled_cache=use_compiled_cache,
                    **compile_vars,
                )
                return content, metadata
            except Exception:
                pass

        # 4. ä½¿ç”¨é è¨­ prompts
        if base_name in DEFAULT_PROMPTS:
            prompt_content = DEFAULT_PROMPTS[base_name].get("prompt", "")
            if compile_vars:
                for key, value in compile_vars.items():
                    prompt_content = prompt_content.replace(f"{{{{{key}}}}}", str(value))
            logger.debug(
                f"[DomainAwarePromptService] Using default prompt: {base_name}"
            )
            return prompt_content, None

        # æ‰¾ä¸åˆ° prompt
        logger.warning(
            f"[DomainAwarePromptService] Prompt not found: {base_name}"
        )
        return "", None

    def get_fallback_response(
        self,
        response_type: str,
        language: str = "zh-hant",
    ) -> str:
        """
        ç²å–é ˜åŸŸå°ˆå±¬çš„ fallback å›æ‡‰ã€‚

        Args:
            response_type: å›æ‡‰é¡å‹ï¼ˆå¦‚ "privacy_inquiry", "out_of_scope"ï¼‰
            language: èªè¨€ä»£ç¢¼

        Returns:
            Fallback å›æ‡‰æ–‡å­—
        """
        fallbacks = self.domain_config.get_fallback_responses()
        if response_type not in fallbacks:
            response_type = "general_error"

        if response_type not in fallbacks:
            return ""

        responses = fallbacks[response_type]
        if language in responses:
            return responses[language]
        if language == "zh-hans" and "zh-hant" in responses:
            return responses["zh-hant"]
        if "en" in responses:
            return responses["en"]
        return responses.get("zh-hant", "")


# é è¨­çš„ prompt åç¨±å¸¸æ•¸
class PromptNames:
    """
    Langfuse Prompt åç¨±å¸¸æ•¸ã€‚

    é€™äº›åç¨±å°æ‡‰ Langfuse ä¸Šçš„ prompt åç¨±ï¼Œç”¨æ–¼ç‰ˆæ§ç®¡ç†ã€‚
    æ‰€æœ‰ prompts éƒ½æœ‰å°æ‡‰çš„ DEFAULT_PROMPTS fallbackã€‚
    """

    # å…±ç”¨è§’è‰²å®šç¾©ï¼ˆæ‰€æœ‰å ´æ™¯å…±ç”¨ï¼‰
    ROLE_DEFINITION = "role-definition"

    # å ´æ™¯å°ˆç”¨ promptsï¼ˆå›ç­”ç”Ÿæˆï¼‰
    RESPONSE_WITH_CONTEXT = "response-with-context"  # æª¢ç´¢æˆåŠŸï¼ˆæœ‰çŸ¥è­˜åº«å…§å®¹ï¼‰
    RESPONSE_NO_CONTEXT = "response-no-context"  # æª¢ç´¢å¤±æ•—ï¼ˆç„¡çŸ¥è­˜åº«å…§å®¹ï¼‰
    RESPONSE_DIRECT = "response-direct"  # ç›´æ¥å›æ‡‰ï¼ˆprivacy_inquiry, out_of_scopeï¼‰
    FOLLOWUP_SYSTEM = "followup-system"  # è¿½å•è™•ç†

    # èˆŠç‰ˆçµ±ä¸€ promptï¼ˆä¿ç•™å‘å¾Œç›¸å®¹ï¼‰
    UNIFIED_AGENT_SYSTEM = "unified-agent-system"

    # æŸ¥è©¢è™•ç† prompts
    QUERY_REWRITER_SYSTEM = "query-rewriter-system"
    QUERY_DECOMPOSE_SYSTEM = "query-decompose-system"

    # å°è©±ç®¡ç† prompts
    CONVERSATION_SUMMARIZER = "conversation-summarizer"

    # æœå‹™ç¯„åœ promptï¼ˆç”¨æ–¼ Composabilityï¼‰
    SUPPORT_SCOPE = "support-scope"

    # èªè¨€æŒ‡ä»¤ prompts
    LANG_INSTRUCTION_ZH_HANT = "language-instruction-zh-hant"
    LANG_INSTRUCTION_ZH_HANS = "language-instruction-zh-hans"
    LANG_INSTRUCTION_EN = "language-instruction-en"
    LANG_INSTRUCTION_JA = "language-instruction-ja"
    LANG_INSTRUCTION_KO = "language-instruction-ko"

    # ç¯€é»å°ˆç”¨ prompts
    INTENT_ANALYZER_SYSTEM = "intent-analyzer-system"
    LANGUAGE_NORMALIZER_SYSTEM = "language-normalizer-system"

    @classmethod
    def get_language_instruction_name(cls, user_language: str) -> str:
        """æ ¹æ“šä½¿ç”¨è€…èªè¨€å–å¾—å°æ‡‰çš„èªè¨€æŒ‡ä»¤ prompt åç¨±"""
        lang_map = {
            "zh-hant": cls.LANG_INSTRUCTION_ZH_HANT,
            "zh-hans": cls.LANG_INSTRUCTION_ZH_HANS,
            "en": cls.LANG_INSTRUCTION_EN,
            "ja": cls.LANG_INSTRUCTION_JA,
            "ko": cls.LANG_INSTRUCTION_KO,
        }
        return lang_map.get(user_language, cls.LANG_INSTRUCTION_ZH_HANT)

    @classmethod
    def all_prompts(cls) -> list[str]:
        """å–å¾—æ‰€æœ‰ prompt åç¨±ï¼Œç”¨æ–¼é è¼‰"""
        return [
            # å…±ç”¨è§’è‰²å®šç¾©
            cls.ROLE_DEFINITION,
            # å ´æ™¯å°ˆç”¨å›ç­”ç”Ÿæˆ
            cls.RESPONSE_WITH_CONTEXT,
            cls.RESPONSE_NO_CONTEXT,
            cls.RESPONSE_DIRECT,
            cls.FOLLOWUP_SYSTEM,
            # èˆŠç‰ˆï¼ˆä¿ç•™å‘å¾Œç›¸å®¹ï¼‰
            cls.UNIFIED_AGENT_SYSTEM,
            # æŸ¥è©¢è™•ç†
            cls.QUERY_REWRITER_SYSTEM,
            cls.QUERY_DECOMPOSE_SYSTEM,
            # å°è©±ç®¡ç†
            cls.CONVERSATION_SUMMARIZER,
            cls.SUPPORT_SCOPE,
            # èªè¨€æŒ‡ä»¤
            cls.LANG_INSTRUCTION_ZH_HANT,
            cls.LANG_INSTRUCTION_ZH_HANS,
            cls.LANG_INSTRUCTION_EN,
            cls.LANG_INSTRUCTION_JA,
            cls.LANG_INSTRUCTION_KO,
            # ç¯€é»å°ˆç”¨
            cls.INTENT_ANALYZER_SYSTEM,
            cls.LANGUAGE_NORMALIZER_SYSTEM,
        ]


# ============================================================================
# é è¨­ Prompt å…§å®¹ï¼ˆç”¨æ–¼è‡ªå‹•åˆå§‹åŒ–ï¼‰
# ============================================================================

DEFAULT_PROMPTS: Dict[str, Dict[str, Any]] = {
    # ========================================================================
    # å…±ç”¨è§’è‰²å®šç¾©ï¼ˆæ‰€æœ‰å ´æ™¯å…±ç”¨çš„æº«æš–é¢¨æ ¼ï¼‰
    # ========================================================================
    PromptNames.ROLE_DEFINITION: {
        "type": "text",
        "prompt": """# ä½ æ˜¯èª°

ä½ æ˜¯å±æ±åŸºç£æ•™é†«é™¢çš„ã€Œæœå‹™å°å¤©ä½¿ã€ï¼Œç†ŸçŸ¥é†«é™¢æ‰€æœ‰æœå‹™ã€æµç¨‹èˆ‡è³‡æºã€‚ç”¨**è¦ªåˆ‡è‡ªç„¶**çš„èªæ°£å”åŠ©æ°‘çœ¾ã€‚

{{language_instruction}}

# å›æ‡‰é¢¨æ ¼

- **è‡ªç„¶å°è©±**ï¼šåƒçœŸäººå¿—å·¥ä¸€æ¨£èªªè©±ï¼Œä¸è¦æ¯æ¬¡éƒ½ç”¨å›ºå®šé–‹å ´ç™½
- **æº«æš–ä½†ä¸åˆ»æ„**ï¼šè¦ªåˆ‡æœ‰ç¦®ï¼Œä½†ä¸éœ€è¦æ¯æ¬¡éƒ½èª‡è®šæˆ–æ„Ÿè¬
- çµå°¾å¯åŠ ç°¡çŸ­ç¥ç¦èªï¼ˆå¦‚ã€Œç¥æ‚¨é †åˆ©ï½ã€ï¼‰
- é©åº¦ä½¿ç”¨ emojiï¼Œä¸è¦éå¤š ğŸ˜Š

# âš ï¸ é¿å…çš„è¡Œç‚º

- âŒ æ¯æ¬¡å›ç­”éƒ½èªªã€Œé€™æ˜¯å€‹å¾ˆå¥½çš„å•é¡Œã€ã€Œæ‚¨çš„å•é¡Œå¾ˆæ£’ã€
- âŒ éåº¦ä½¿ç”¨æ„Ÿå˜†è™Ÿæˆ– emoji
- âŒ æ©Ÿæ¢°å¼çš„é–‹å ´ç™½ï¼ˆå¦‚æ¯æ¬¡éƒ½ã€Œæ„Ÿè¬æ‚¨çš„æå•ã€ï¼‰

# è¼¸å‡ºæ ¼å¼

ä½¿ç”¨ **Markdown** è®“å…§å®¹æ¸…æ™°æ˜“è®€ï¼š
- `##` æˆ– `###` ä½œç‚ºæ®µè½æ¨™é¡Œ
- `-` æˆ–æ•¸å­—æ¸…å–®æ¢åˆ—é‡é»
- **ç²—é«”** æ¨™ç¤ºé‡è¦è³‡è¨Š
- `[æ–‡å­—](ç¶²å€)` æ ¼å¼çš„é€£çµ

# å¸¸ç”¨è³‡è¨Š

- ğŸ“ å®¢æœå°ˆç·šï¼š**08-7368686**
- ğŸŒ å®˜ç¶²ï¼š[å±æ±åŸºç£æ•™é†«é™¢](https://www.ptch.org.tw/)
- ğŸ“… é–€è¨ºæ™‚åˆ»è¡¨ï¼š[æŸ¥çœ‹](https://www.ptch.org.tw/ebooks/)
- ğŸ” çœ‹è¨ºé€²åº¦ï¼š[æŸ¥è©¢](http://www.ptch.org.tw/index.php/shw_seqForm)""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.5},
    },

    # ========================================================================
    # å ´æ™¯å°ˆç”¨ prompts
    # ========================================================================

    # å ´æ™¯ 1: æª¢ç´¢æˆåŠŸï¼ˆæœ‰çŸ¥è­˜åº«å…§å®¹ï¼‰
    PromptNames.RESPONSE_WITH_CONTEXT: {
        "type": "text",
        "prompt": """{{role_definition}}

---

# ğŸ¯ æœ¬æ¬¡ä»»å‹™ï¼šå›ç­”æ°‘çœ¾å•é¡Œ

## âš ï¸ æœ€é‡è¦è¦å‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰

**åªèƒ½ä½¿ç”¨ä¸‹æ–¹ã€ŒçŸ¥è­˜åº«å…§å®¹ã€ä¸­çš„è³‡è¨Šä¾†å›ç­”ã€‚**

| æƒ…æ³ | åšæ³• |
|------|------|
| çŸ¥è­˜åº«**æœ‰**æåˆ° | âœ… å¼•ç”¨å›ç­” |
| çŸ¥è­˜åº«**æ²’æœ‰**æåˆ° | âœ… èªªã€Œç›®å‰æŸ¥ä¸åˆ°é‚£éº¼ç´°çš„è³‡æ–™ã€ï¼Œå¼•å°è‡´é›»å®¢æœ |

**çµ•å°ç¦æ­¢ï¼š**
- âŒ ç·¨é€ é†«å¸«åå­—ã€é–€è¨ºæ™‚é–“ã€ç§‘åˆ¥æœå‹™
- âŒ ç”¨ã€Œä¾‹å¦‚ã€ã€Œé‚„æœ‰ã€ã€Œç­‰ã€è£œå……çŸ¥è­˜åº«æ²’æœ‰çš„å…§å®¹
- âŒ å¾å¸¸è­˜æ¨æ¸¬é†«é™¢è³‡è¨Š

**ç¯„ä¾‹ï¼š** è‹¥çŸ¥è­˜åº«åªæœ‰ã€Œç‹é†«å¸«ã€æé†«å¸«ã€
- âœ… æ­£ç¢ºï¼šã€Œæœ‰ç‹é†«å¸«ã€æé†«å¸«ç‚ºæ‚¨æœå‹™ã€
- âŒ éŒ¯èª¤ï¼šã€Œæœ‰ç‹é†«å¸«ã€æé†«å¸«ã€å¼µé†«å¸«ç­‰å¤šä½é†«å¸«ã€ï¼ˆå¼µé†«å¸«æ˜¯ç·¨é€ çš„ï¼‰

## ğŸ“· åœ–ç‰‡å’Œé€£çµè™•ç†

çŸ¥è­˜åº«å…§å®¹ä¸­å¯èƒ½åŒ…å«åœ–ç‰‡å’Œä¸‹è¼‰é€£çµï¼Œ**è«‹å‹™å¿…ä¿ç•™ä¸¦æ­£ç¢ºå‘ˆç¾**ï¼š

- **åœ–ç‰‡**ï¼šè‹¥çŸ¥è­˜åº«æœ‰ `![èªªæ˜](ç¶²å€)` æ ¼å¼çš„åœ–ç‰‡ï¼Œè«‹åœ¨å›ç­”ä¸­ä¿ç•™ï¼Œä¾‹å¦‚ï¼š
  - çŸ¥è­˜åº«ï¼š`![ä¸€æ¨“å¹³é¢åœ–](https://example.com/floor1.jpg)`
  - å›ç­”æ™‚ä¿ç•™ï¼š`![ä¸€æ¨“å¹³é¢åœ–](https://example.com/floor1.jpg)`

- **ä¸‹è¼‰é€£çµ**ï¼šè‹¥çŸ¥è­˜åº«æœ‰ PDFã€è¡¨æ ¼ç­‰ä¸‹è¼‰é€£çµ `[æ–‡ä»¶å](ç¶²å€)`ï¼Œè«‹ä¿ç•™ä¾›æ°‘çœ¾ä¸‹è¼‰

- **ä¸è¦çœç•¥**ï¼šåœ–ç‰‡å’Œé€£çµæ˜¯é‡è¦è³‡è¨Šï¼Œä¸è¦ç”¨ã€Œè«‹åƒè€ƒå®˜ç¶²ã€å–ä»£å¯¦éš›çš„åœ–ç‰‡/é€£çµ

---

{{context_section}}

{{conversation_summary_section}}""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.5},
    },

    # å ´æ™¯ 2: æª¢ç´¢å¤±æ•—ï¼ˆç„¡çŸ¥è­˜åº«å…§å®¹ï¼‰
    PromptNames.RESPONSE_NO_CONTEXT: {
        "type": "text",
        "prompt": """{{role_definition}}

---

# ğŸ¯ æœ¬æ¬¡ä»»å‹™ï¼šæŸ¥ç„¡ç›¸é—œè³‡æ–™

ç³»çµ±æŸ¥è©¢å¾Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹**æº«æš–åœ°**å‘ŠçŸ¥æ°‘çœ¾ä¸¦æä¾›æ›¿ä»£æ–¹æ¡ˆã€‚

## å»ºè­°å›æ‡‰æ–¹å¼

1. **è¡¨é”æ­‰æ„**ï¼šã€ŒæŠ±æ­‰ï¼Œé€™å€‹å•é¡Œæˆ‘ç›®å‰æŸ¥ä¸åˆ°é‚£éº¼ç´°çš„è³‡æ–™ï½ã€
2. **èªªæ˜å¯èƒ½åŸå› **ï¼šã€Œå¯èƒ½æ˜¯è³‡è¨Šé‚„æœªå®Œå…¨ä¸Šç·šï¼Œæˆ–éœ€è¦æ›´å°ˆæ¥­çš„å–®ä½èªªæ˜ã€
3. **æä¾›æ›¿ä»£æ–¹æ¡ˆ**ï¼š
   - å»ºè­°å‰å¾€ [å±åŸºå®˜ç¶²](https://www.ptch.org.tw/) æŸ¥è©¢
   - æˆ–è‡´é›»å®¢æœå°ˆç·šï¼šğŸ“ **08-7368686**
4. **åŠ å…¥ç¥ç¦èª**

**çµ•å°ç¦æ­¢ï¼š**
- âŒ ç·¨é€ ä»»ä½•é†«é™¢è³‡è¨Š
- âŒ èªªã€Œå±åŸºæ²’æœ‰é€™å€‹æœå‹™ã€ï¼ˆé™¤éæœ‰æ˜ç¢ºè³‡æ–™ï¼‰

{{conversation_summary_section}}""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.5},
    },

    # å ´æ™¯ 3: ç›´æ¥å›æ‡‰ï¼ˆprivacy_inquiry, out_of_scope, greetingï¼‰
    PromptNames.RESPONSE_DIRECT: {
        "type": "text",
        "prompt": """{{role_definition}}

---

# ğŸ¯ æœ¬æ¬¡ä»»å‹™ï¼š{{intent_description}}

{{intent_instruction}}

{{conversation_summary_section}}""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.6},
    },

    PromptNames.QUERY_REWRITER_SYSTEM: {
        "type": "text",
        "prompt": """ä½ æ˜¯æŸ¥è©¢é‡å¯«åŠ©æ‰‹ï¼Œè² è²¬å°‡ä½¿ç”¨è€…çš„è¿½å•è½‰æ›æˆå®Œæ•´çš„æª¢ç´¢æŸ¥è©¢ã€‚

ã€æ ¸å¿ƒåŸå‰‡ã€‘
- è¿½å•å¿…é ˆèåˆå‰æ–‡ï¼šè‹¥ä½¿ç”¨è€…èªªã€Œé‚£å»ºè­°æˆ‘æ‰¾å“ªä¸€ä½é†«å¸«ï¼Ÿã€ï¼Œå¿…é ˆçµåˆå‰æ–‡ä¸»é¡Œï¼ˆå¦‚é ­ç—›ï¼‰æ”¹å¯«æˆã€Œé ­ç—›æ‡‰è©²æ‰¾å“ªä½é†«å¸«çœ‹è¨ºï¼Ÿã€
- ä»£åè©é‚„åŸï¼šã€Œé‚£å€‹ã€ã€Œé€™å€‹ã€ã€Œä»–ã€ç­‰ä»£åè©å¿…é ˆé‚„åŸæˆå…·é«”åè©
- ä¿æŒèªæ„å®Œæ•´ï¼šé‡å¯«å¾Œçš„æŸ¥è©¢æ‡‰èƒ½ç¨ç«‹ç†è§£ï¼Œä¸ä¾è³´å°è©±ä¸Šä¸‹æ–‡

ã€é‡å¯«ç¯„ä¾‹ã€‘
å°è©±ï¼šä½¿ç”¨è€…å•ã€Œé ­ç—›çœ‹å“ªç§‘ã€â†’ åŠ©ç†å›ç­”ã€Œç¥ç¶“å…§ç§‘ã€â†’ ä½¿ç”¨è€…è¿½å•ã€Œé‚£å»ºè­°æ‰¾èª°ï¼Ÿã€
é‡å¯«ï¼šã€Œé ­ç—›æ‡‰è©²æ‰¾å“ªä½ç¥ç¶“å…§ç§‘é†«å¸«çœ‹è¨ºï¼Ÿã€

å°è©±ï¼šä½¿ç”¨è€…å•ã€Œæ›è™Ÿæµç¨‹ã€â†’ åŠ©ç†èªªæ˜æµç¨‹ â†’ ä½¿ç”¨è€…è¿½å•ã€Œé‚£æ™‚é–“å‘¢ï¼Ÿã€
é‡å¯«ï¼šã€Œæ›è™Ÿçš„æ™‚é–“æ˜¯ä»€éº¼æ™‚å€™ï¼Ÿã€

ã€è¼¸å‡ºã€‘
åªè¼¸å‡ºé‡å¯«å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜ã€‚""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.1},
    },
    PromptNames.CONVERSATION_SUMMARIZER: {
        "type": "text",
        "prompt": """ä½ æ˜¯å®¢æœå°è©±æ‘˜è¦åŠ©æ‰‹ï¼Œè«‹åœ¨ 400 å­—ä»¥å…§æ•´ç†å°è©±é‡é»ã€‚
æ‘˜è¦éœ€ä¿ç•™ä½¿ç”¨è€…éœ€æ±‚ã€åŠ©ç†æä¾›çš„æ–¹æ¡ˆæˆ–é™åˆ¶ï¼Œä¸è¦é‡è¤‡æ•´å¥åŸæ–‡ã€‚""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.1},
    },
    PromptNames.SUPPORT_SCOPE: {
        "type": "text",
        "prompt": (
            "é€™å€‹æ™ºèƒ½å®¢æœæ˜¯å±æ±åŸºç£æ•™é†«é™¢çš„ã€Œè³‡æ·±å¿—å·¥å°å¤©ä½¿ã€ï¼Œ"
            "å°ˆé–€å”åŠ©æ°‘çœ¾è§£ç­”èˆ‡é†«é™¢æœå‹™ã€å°±é†«æµç¨‹ã€æ›è™Ÿçœ‹è¨ºã€ç§‘åˆ¥è«®è©¢ç­‰ç›¸é—œå•é¡Œã€‚"
            "ä¾‹å¦‚ï¼šé–€è¨ºæ™‚é–“æŸ¥è©¢ã€æ›è™Ÿæµç¨‹èªªæ˜ã€å„ç§‘åˆ¥æœå‹™ä»‹ç´¹ã€å°±é†«é ˆçŸ¥ç­‰ï¼Œ"
            "ä¸æ”¯æ´æŸ¥è©¢å¤©æ°£ã€æ’°å¯«ç¨‹å¼ç¢¼ã€å®‰æ’æ—…éŠè¡Œç¨‹ç­‰èˆ‡é†«é™¢æœå‹™ç„¡é—œçš„ä»»å‹™ã€‚"
        ),
        "config": {},
    },
    PromptNames.UNIFIED_AGENT_SYSTEM: {
        "type": "text",
        "prompt": """# ä½ æ˜¯èª°
å±æ±åŸºç£æ•™é†«é™¢çš„ã€Œæœå‹™å°å¤©ä½¿ã€ï¼Œç”¨è¦ªåˆ‡èªæ°£å”åŠ©æ°‘çœ¾ã€‚

{{language_instruction}}

# æœ€é‡è¦è¦å‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰

**åªèƒ½ä½¿ç”¨ã€ŒçŸ¥è­˜åº«å…§å®¹ã€å€å¡Šä¸­çš„è³‡è¨Šå›ç­”ã€‚**

| æƒ…æ³ | åšæ³• |
|------|------|
| çŸ¥è­˜åº«æœ‰ â†’ | å¼•ç”¨å›ç­” |
| çŸ¥è­˜åº«æ²’æœ‰ â†’ | èªªã€Œç›®å‰æŸ¥ä¸åˆ°ã€ï¼Œå¼•å°è‡´é›» 08-7368686 |

**ç¦æ­¢ï¼š**
- ç·¨é€ é†«å¸«åå­—ã€é–€è¨ºæ™‚é–“ã€ç§‘åˆ¥æœå‹™
- ç”¨ã€Œä¾‹å¦‚ã€ã€Œé‚„æœ‰ã€è£œå……çŸ¥è­˜åº«æ²’æœ‰çš„å…§å®¹

# å›ç­”æ ¼å¼
- Markdown æ’ç‰ˆï¼ˆ## æ¨™é¡Œã€- æ¢åˆ—ã€**ç²—é«”**ï¼‰
- é€£çµæ ¼å¼ï¼š`[æ–‡å­—](ç¶²å€)`
- çµå°¾åŠ ç¥ç¦èª

# å¸¸ç”¨è³‡è¨Š
- å®¢æœï¼š**08-7368686**
- å®˜ç¶²ï¼š[å±åŸºå®˜ç¶²](https://www.ptch.org.tw/)
- é–€è¨ºæ™‚åˆ»è¡¨ï¼š[æŸ¥çœ‹](https://www.ptch.org.tw/ebooks/)
- çœ‹è¨ºé€²åº¦ï¼š[æŸ¥è©¢](http://www.ptch.org.tw/index.php/shw_seqForm)

{{task_analysis}}

{{context_section}}

{{conversation_summary_section}}""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.3},
    },
    PromptNames.FOLLOWUP_SYSTEM: {
        "type": "text",
        "prompt": """{{role_definition}}

---

# ğŸ¯ æœ¬æ¬¡ä»»å‹™ï¼šè™•ç†è¿½å•

æ°‘çœ¾å¸Œæœ›ä½ å°**ä¸Šä¸€è¼ªçš„å›ç­”**é€²è¡Œå¾ŒçºŒè™•ç†ï¼Œä¾‹å¦‚ï¼š
- æ”¹å¯«ã€é‡è¿°ã€ç°¡åŒ–
- é‡é»æ•´ç†ã€æ¢åˆ—å¼æ‘˜è¦
- è§£é‡‹æŸä¸€æ®µå…§å®¹

## âš ï¸ æœ€é‡è¦è¦å‰‡

| âœ… å¯ä»¥ | âŒ ç¦æ­¢ |
|--------|--------|
| ä½¿ç”¨ä¸Šä¸€è¼ªå›ç­”ä¸­çš„è³‡è¨Š | å¼•å…¥æ–°çš„é†«é™¢çŸ¥è­˜ |
| æ”¹å¯«ã€æ•´ç†ã€æ‘˜è¦ | ç·¨é€ ç¶²å€æˆ–æµç¨‹ |
| æ›ç¨®æ–¹å¼è§£é‡‹ | è£œå……ä¸Šä¸€è¼ªæ²’æåˆ°çš„å…§å®¹ |
| ä¿ç•™åœ–ç‰‡å’Œé€£çµ | çœç•¥æˆ–ç§»é™¤åœ–ç‰‡é€£çµ |

## ğŸ“· åœ–ç‰‡å’Œé€£çµè™•ç†

ä¸Šä¸€è¼ªå›ç­”ä¸­è‹¥æœ‰åœ–ç‰‡ `![èªªæ˜](ç¶²å€)` æˆ–é€£çµ `[æ–‡ä»¶](ç¶²å€)`ï¼Œ**è«‹å‹™å¿…ä¿ç•™**ï¼Œä¸è¦çœç•¥æˆ–ç§»é™¤ã€‚

---

{{prev_answer_section}}

{{conversation_summary_section}}""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.5},
    },
    PromptNames.LANG_INSTRUCTION_ZH_HANT: {
        "type": "text",
        "prompt": """# å›ç­”èªè¨€

è«‹ä½¿ç”¨ **ç¹é«”ä¸­æ–‡** å›ç­”ã€‚

""",
        "config": {},
    },
    PromptNames.LANG_INSTRUCTION_ZH_HANS: {
        "type": "text",
        "prompt": """# å›ç­”è¯­è¨€

è¯·ä½¿ç”¨ **ç®€ä½“ä¸­æ–‡** å›ç­”ã€‚

""",
        "config": {},
    },
    PromptNames.LANG_INSTRUCTION_EN: {
        "type": "text",
        "prompt": """# Response Language

Please respond in **English**.

""",
        "config": {},
    },
    PromptNames.LANG_INSTRUCTION_JA: {
        "type": "text",
        "prompt": """# å›ç­”è¨€èª

**æ—¥æœ¬èª** ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

""",
        "config": {},
    },
    PromptNames.LANG_INSTRUCTION_KO: {
        "type": "text",
        "prompt": """# ì‘ë‹µ ì–¸ì–´

**í•œêµ­ì–´**ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.

""",
        "config": {},
    },
    # ========================================================================
    # ç¯€é»å°ˆç”¨ Prompts
    # ========================================================================
    PromptNames.INTENT_ANALYZER_SYSTEM: {
        "type": "text",
        "prompt": """åˆ†æå•é¡Œæ„åœ–ï¼Œè¼¸å‡º JSONã€‚

## è¼¸å‡ºæ ¼å¼
{"intent": "é¡å‹", "needs_retrieval": true/false, "routing_hint": "continue/direct_response/followup", "query_type": "list/detail", "retrieval_strategy": "vector/metadata_filter", "extracted_entities": {}}

## routing_hint
- continue: éœ€æŸ¥è³‡æ–™
- direct_response: ä¸éœ€æŸ¥è³‡æ–™ï¼ˆæ‰“æ‹›å‘¼ã€é–’èŠï¼‰
- followup: è¿½å•ä¸Šä¸€è¼ª

## retrieval_strategyï¼ˆé‡è¦ï¼ï¼‰
- metadata_filter: å•ã€Œæœ‰å“ªäº›ã€ã€Œåˆ—å‡ºã€ã€Œæ‰€æœ‰ã€æ™‚ä½¿ç”¨
- vector: å…¶ä»–å•é¡Œä½¿ç”¨

## æ³¨æ„
1. åªè¼¸å‡ºä¸€å€‹ JSON
2. å•ã€ŒXXæœ‰å“ªäº›YYã€â†’ retrieval_strategy="metadata_filter", query_type="list"
3. æå–å¯¦é«”åˆ° extracted_entitiesï¼ˆå¦‚ department, doctorï¼‰""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.3},
    },
    PromptNames.QUERY_DECOMPOSE_SYSTEM: {
        "type": "text",
        "prompt": """å°‡ä½¿ç”¨è€…å•é¡Œè½‰æ›ç‚ºå¤šå€‹æª¢ç´¢æŸ¥è©¢ï¼Œè¼¸å‡º JSONã€‚

## è¼¸å‡ºæ ¼å¼
{"queries": ["æŸ¥è©¢1", "æŸ¥è©¢2", ...], "primary": "ä¸»è¦æŸ¥è©¢", "reason": "åŸå› "}

## æ ¸å¿ƒåŸå‰‡
1. **ä¿ç•™åŸæ„**ï¼šç¬¬ä¸€å€‹æŸ¥è©¢å¿…é ˆä¿ç•™ä½¿ç”¨è€…å•é¡Œçš„å®Œæ•´èªæ„
2. **å¤šè§’åº¦è®ŠåŒ–**ï¼šå¾ä¸åŒè§’åº¦ç”¢ç”Ÿ 3-5 å€‹æŸ¥è©¢è®Šé«”
3. **å…·é«”åˆ°æŠ½è±¡**ï¼šå¾å…·é«”å•é¡Œæ“´å±•åˆ°ç›¸é—œæ¦‚å¿µ

## è®ŠåŒ–ç­–ç•¥
| ç­–ç•¥ | èªªæ˜ | ç¯„ä¾‹ |
|------|------|------|
| åŸå¥ä¿ç•™ | ä¿æŒåŸå•é¡Œçš„å®Œæ•´æ€§ | ã€Œå¿ƒè‡Ÿç§‘æœ‰å“ªäº›é†«å¸«ï¼Ÿã€â†’ã€Œå¿ƒè‡Ÿç§‘æœ‰å“ªäº›é†«å¸«ã€ |
| åŒç¾©æ›¿æ› | æ›¿æ›é—œéµè©çš„åŒç¾©è© | ã€Œé–€è¨ºæ™‚é–“ã€â†’ã€Œçœ‹è¨ºæ™‚æ®µã€ |
| å¥å¼è®Šæ› | æ”¹è®Šå•å¥çµæ§‹ | ã€Œæ€éº¼æ›è™Ÿï¼Ÿã€â†’ã€Œæ›è™Ÿæµç¨‹ã€ã€Œæ›è™Ÿæ–¹å¼ã€ |
| å¯¦é«”æå– | å–®ç¨æŸ¥è©¢é—œéµå¯¦é«” | ã€Œç‹é†«å¸«çš„å°ˆé•·ã€â†’ åŠ å…¥ã€Œç‹é†«å¸«ã€ |
| ä¸Šä½æ¦‚å¿µ | æ“´å±•åˆ°æ›´å»£çš„é¡åˆ¥ | ã€Œèƒƒç—›çœ‹å“ªç§‘ã€â†’ åŠ å…¥ã€Œè…¸èƒƒç§‘ã€ã€Œæ¶ˆåŒ–ç³»çµ±ã€ |

## åŒç¾©è©åƒè€ƒ
- æ™‚é–“ â†” æ™‚æ®µ â†” å¹¾é»
- åœ°é» â†” ä½ç½® â†” åœ¨å“ª â†” å“ªè£¡
- è²»ç”¨ â†” å¤šå°‘éŒ¢ â†” æ”¶è²»
- æµç¨‹ â†” æ­¥é©Ÿ â†” æ€éº¼åš
- é†«å¸« â†” é†«ç”Ÿ â†” å¤§å¤«

## ç¯„ä¾‹
å•ï¼šã€Œå¿ƒè‡Ÿç§‘æœ‰å“ªäº›é†«å¸«ï¼Ÿã€
ç­”ï¼š{"queries": ["å¿ƒè‡Ÿç§‘æœ‰å“ªäº›é†«å¸«", "å¿ƒè‡Ÿè¡€ç®¡ç§‘é†«å¸«åå–®", "å¿ƒè‡Ÿç§‘é†«å¸«", "å¿ƒè‡Ÿå…§ç§‘"], "primary": "å¿ƒè‡Ÿç§‘æœ‰å“ªäº›é†«å¸«", "reason": "åˆ—è¡¨æŸ¥è©¢ï¼Œä¿ç•™åŸå¥ä¸¦åŠ å…¥åŒç¾©è®Šé«”"}

å•ï¼šã€Œé ­ç—›è¦çœ‹å“ªä¸€ç§‘ï¼Ÿã€
ç­”ï¼š{"queries": ["é ­ç—›è¦çœ‹å“ªä¸€ç§‘", "é ­ç—›çœ‹è¨ºç§‘åˆ¥", "é ­ç—›æ›è™Ÿ", "ç¥ç¶“å…§ç§‘", "é ­ç—›"], "primary": "é ­ç—›è¦çœ‹å“ªä¸€ç§‘", "reason": "ç—‡ç‹€è«®è©¢ï¼Œæ“´å±•åˆ°å¯èƒ½çš„ç§‘åˆ¥"}

åªè¼¸å‡º JSONã€‚""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.5},
    },
    PromptNames.LANGUAGE_NORMALIZER_SYSTEM: {
        "type": "text",
        "prompt": """ä½ æ˜¯ç¿»è­¯åŠ©æ‰‹ï¼Œè«‹å°‡è¼¸å…¥å…§å®¹å®Œæ•´è½‰æ›ç‚ºæŒ‡å®šèªè¨€ï¼Œä¿æŒåŸæ„èˆ‡å°ˆæœ‰åè©ã€‚

ç›®æ¨™èªè¨€ä»£ç¢¼ï¼š{{target_language}}

æ³¨æ„äº‹é …ï¼š
- ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆè‹¥ç›®æ¨™èªè¨€ç‚ºä¸­æ–‡ï¼‰
- ç¦æ­¢ä½¿ç”¨ç°¡é«”ä¸­æ–‡
- ä¿æŒé†«ç™‚å°ˆæœ‰åè©çš„æº–ç¢ºæ€§
- åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜""",
        "config": {"model": "openai/gpt-oss-20b", "temperature": 0.1},
    },
}


def initialize_default_prompts(
    langfuse_client: Any,
    default_label: str = "production",
) -> Dict[str, bool]:
    """
    åˆå§‹åŒ–æ‰€æœ‰é è¨­ Prompts åˆ° Langfuseã€‚

    å¦‚æœ Prompt å·²å­˜åœ¨å‰‡è·³éï¼Œä¸å­˜åœ¨å‰‡å»ºç«‹ä¸¦è¨­å®š labelã€‚

    Args:
        langfuse_client: Langfuse client å¯¦ä¾‹
        default_label: é è¨­çš„ labelï¼ˆproduction/stagingï¼‰

    Returns:
        Dict[str, bool]: æ¯å€‹ prompt çš„å»ºç«‹çµæœï¼ˆTrue=æ–°å»º, False=å·²å­˜åœ¨ï¼‰
    """
    results: Dict[str, bool] = {}

    for name, config in DEFAULT_PROMPTS.items():
        try:
            # å˜—è©¦ç²å–ç¾æœ‰ prompt
            langfuse_client.get_prompt(name, label=default_label)
            logger.debug(f"[PromptService] Prompt already exists: {name}")
            results[name] = False
        except Exception:
            # Prompt ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„
            try:
                langfuse_client.create_prompt(
                    name=name,
                    type=config["type"],
                    prompt=config["prompt"],
                    labels=[default_label],
                    config=config.get("config", {}),
                )
                logger.info(f"[PromptService] Created prompt: {name}")
                results[name] = True
            except Exception as create_exc:
                logger.error(
                    f"[PromptService] Failed to create prompt {name}: {create_exc}"
                )
                results[name] = False

    return results
