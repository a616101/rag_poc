"""
è¼¸å‡ºç¯€é»

æœ€çµ‚ç­”æ¡ˆç”Ÿæˆå’Œé™æ¸¬ã€‚
å¯¦ç¾ OWASP LLM02ï¼ˆè¼¸å‡ºè™•ç†ï¼‰ç·©è§£æªæ–½ã€‚

ä¸»è¦ç¯€é»ï¼š
- final_answer_node: ä½¿ç”¨ LLM ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
- telemetry_node: è¨˜éŒ„è¿½è¹¤è³‡è¨Šåˆ° Langfuse
"""

import html
import logging
import re
from typing import Any

from chatbot_graphrag.graph_workflow.types import GraphRAGState

logger = logging.getLogger(__name__)

# æœ€å¤§è¼¸å‡ºé•·åº¦ä»¥é˜²æ­¢è³‡æºè€—ç›¡
MAX_OUTPUT_LENGTH = 10000

# è¦å¾è¼¸å‡ºä¸­å‰é›¢çš„æ¨¡å¼ï¼ˆå…§éƒ¨æ¨™è¨˜ã€ç³»çµ±æç¤ºï¼‰
OUTPUT_STRIP_PATTERNS = [
    r"<\|system\|>.*?<\|/system\|>",
    r"<system>.*?</system>",
    r"\[INTERNAL\].*?\[/INTERNAL\]",
    r"###\s*System:.*?(?=###|$)",
    r"DEBUG:.*?(?:\n|$)",
]

COMPILED_OUTPUT_STRIP = [re.compile(p, re.IGNORECASE | re.DOTALL) for p in OUTPUT_STRIP_PATTERNS]


def sanitize_output(text: str, escape_html: bool = True) -> str:
    """
    æ¸…ç† LLM è¼¸å‡ºä»¥å®‰å…¨é¡¯ç¤ºï¼ˆOWASP LLM02ï¼‰ã€‚

    Args:
        text: åŸå§‹ LLM è¼¸å‡º
        escape_html: æ˜¯å¦è·³è„« HTML å¯¦é«”

    Returns:
        å®‰å…¨ç”¨æ–¼å‰ç«¯é¡¯ç¤ºçš„æ¸…ç†éçš„æ–‡å­—
    """
    if not text:
        return text

    # 1. å‰é›¢å…§éƒ¨æ¨™è¨˜å’Œæ´©æ¼çš„ç³»çµ±æç¤º
    for pattern in COMPILED_OUTPUT_STRIP:
        text = pattern.sub("", text)

    # 2. å¦‚æœå•Ÿç”¨å‰‡è·³è„« HTML å¯¦é«”ï¼ˆé˜²æ­¢ XSSï¼‰
    if escape_html:
        text = html.escape(text)

    # 3. é™åˆ¶è¼¸å‡ºé•·åº¦
    if len(text) > MAX_OUTPUT_LENGTH:
        text = text[:MAX_OUTPUT_LENGTH] + "\n...(å›ç­”å·²æˆªæ–·)"
        logger.warning(f"Output truncated from {len(text)} to {MAX_OUTPUT_LENGTH} chars")

    # 4. æ¸…ç†éå¤šçš„ç©ºç™½
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = text.strip()

    return text


async def final_answer_node(state: GraphRAGState, config: dict | None = None) -> dict[str, Any]:
    """
    æœ€çµ‚ç­”æ¡ˆç”Ÿæˆç¯€é»ã€‚

    ä½¿ç”¨ LLM èˆ‡ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆã€‚

    Args:
        state: ç•¶å‰åœ–è­œç‹€æ…‹
        config: åŒ…å« Langfuse è¿½è¹¤å›èª¿çš„å¯é¸ LangGraph é…ç½®

    Returns:
        æ›´æ–°å¾Œçš„ç‹€æ…‹ï¼ŒåŒ…å« final_answer å’Œå¼•ç”¨
    """
    import time

    # å¾é…ç½®ä¸­æå– Langfuse è¿½è¹¤çš„å›èª¿
    callbacks = config.get("callbacks", []) if config else []

    start_time = time.time()
    # å„ªå…ˆä½¿ç”¨ resolved_questionï¼ˆä¾†è‡ªå¸¶æœ‰è¿½è¹¤ä¸Šä¸‹æ–‡çš„æŸ¥è©¢åˆ†è§£å™¨ï¼‰
    # è€Œé normalized_question å’ŒåŸå§‹å•é¡Œ
    question = (
        state.get("resolved_question")
        or state.get("normalized_question")
        or state.get("question", "")
    )
    user_language = state.get("user_language", "zh-TW")
    context_text = state.get("context_text", "")
    evidence_table = state.get("evidence_table", [])
    groundedness_score = state.get("groundedness_score", 0.0)
    retrieval_path = list(state.get("retrieval_path", []))
    timing = dict(state.get("timing", {}))

    # è™•ç†è¢«é˜»æ“‹/æ‹’çµ•çš„æƒ…æ³
    if state.get("guard_blocked"):
        reason = state.get("guard_reason", "å®‰å…¨æª¢æŸ¥æœªé€šé")
        answer = f"""å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†æ‚¨çš„å•é¡Œ ğŸ™

**åŸå› ï¼š** {reason}

---

å¦‚æœæ‚¨èªç‚ºé€™æ˜¯èª¤åˆ¤ï¼Œæˆ–æœ‰å…¶ä»–å•é¡Œéœ€è¦å”åŠ©ï¼Œæ­¡è¿é‡æ–°æå•æˆ–è¯ç¹«æˆ‘å€‘çš„å®¢æœäººå“¡ã€‚æˆ‘å€‘æœƒç›¡åŠ›å¹«åŠ©æ‚¨ï¼"""
        return {
            "final_answer": answer,
            "confidence": 0.0,
            "citations": [],
            "retrieval_path": retrieval_path + ["final_answer:blocked"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }

    if state.get("acl_denied"):
        answer = """å¾ˆæŠ±æ­‰ï¼Œæ‚¨ç›®å‰æ²’æœ‰æ¬Šé™è¨ªå•ç›¸é—œè³‡è¨Š ğŸ”’

---

å¦‚éœ€å–å¾—ç›¸é—œè³‡æ–™çš„å­˜å–æ¬Šé™ï¼Œè«‹è¯ç¹«æ‚¨çš„ç®¡ç†å“¡æˆ–å®¢æœäººå“¡ã€‚

æœ‰å…¶ä»–å•é¡Œæ­¡è¿éš¨æ™‚è©¢å•ï¼Œæˆ‘æœƒç›¡åŠ›å”åŠ©æ‚¨ï¼"""
        return {
            "final_answer": answer,
            "confidence": 0.0,
            "citations": [],
            "retrieval_path": retrieval_path + ["final_answer:acl_denied"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }

    # è™•ç† HITL é€¾æ™‚ï¼ˆç¬¬ 3 éšæ®µï¼‰
    hitl_timeout_at = state.get("hitl_timeout_at")
    if hitl_timeout_at and state.get("hitl_required") and not state.get("hitl_resolved"):
        import time as time_module

        if time_module.time() > hitl_timeout_at:
            from chatbot_graphrag.graph_workflow.nodes.quality import get_hitl_fallback_response

            answer = get_hitl_fallback_response(state)
            logger.warning("HITL timeout - using fallback response")
            return {
                "final_answer": answer,
                "confidence": 0.0,
                "citations": [],
                "hitl_timed_out": True,
                "retrieval_path": retrieval_path + ["final_answer:hitl_timeout"],
                "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
            }

    # è™•ç† HITL æ‹’çµ•ï¼ˆç¬¬ 3 éšæ®µï¼‰
    if state.get("hitl_approved") is False:
        if user_language.startswith("zh"):
            answer = """æ‚¨çš„å•é¡Œå·²ç¶“ç”±æˆ‘å€‘çš„å¯©æ ¸äººå“¡ä»”ç´°è™•ç† ğŸ“‹

---

å¾ˆæŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•æä¾›å®Œæ•´çš„å›ç­”ã€‚

### å»ºè­°æ‚¨å¯ä»¥ï¼š
- è¯ç¹«æˆ‘å€‘çš„å®¢æœäººå“¡å–å¾—å°ˆäººå”åŠ©
- è¦ªè‡ªåˆ°é†«é™¢æœå‹™å°æ´½è©¢

æ„Ÿè¬æ‚¨çš„è€å¿ƒç­‰å¾…ï¼Œæˆ‘å€‘æœƒæŒçºŒæ”¹é€²æœå‹™å“è³ªï¼"""
        else:
            answer = """Your question was carefully reviewed by our team ğŸ“‹

---

Unfortunately, we couldn't provide a complete answer at this time.

### We suggest you:
- Contact our support team for personalized assistance
- Visit the hospital service desk in person

Thank you for your patience. We're continuously improving our services!"""

        return {
            "final_answer": answer,
            "confidence": 0.0,
            "citations": [],
            "hitl_rejected": True,
            "retrieval_path": retrieval_path + ["final_answer:hitl_rejected"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }

    # æª¢æŸ¥æˆ‘å€‘æ˜¯å¦å·²ç¶“æœ‰ç­”æ¡ˆï¼ˆä¾†è‡ª direct_answer æˆ–å¿«å–ï¼‰
    existing_answer = state.get("final_answer", "")
    if existing_answer:
        # é€šé SSE äº‹ä»¶å°‡å¿«å–çš„ç­”æ¡ˆä¸²æµåˆ°å‰ç«¯
        # é€™ç¢ºä¿å‰ç«¯å³ä½¿å°æ–¼å¿«å–çš„å›æ‡‰ä¹Ÿèƒ½æ¥æ”¶å…§å®¹
        import asyncio
        logger.info(f"Found existing answer ({len(existing_answer)} chars), attempting to stream...")

        try:
            from langgraph.config import get_stream_writer

            writer = get_stream_writer()

            if writer is None:
                logger.warning("get_stream_writer() returned None - streaming not available")
            else:
                logger.debug(f"Got stream writer: {type(writer)}")

                # ç™¼é€é–‹å§‹ç‹€æ…‹
                writer({
                    "node": "final_answer",
                    "channel": "status",
                    "stage": "GENERATING",
                })

                # åˆ†å¡Šä¸²æµå¿«å–çš„ç­”æ¡ˆä»¥å¯¦ç¾å¹³æ»‘é¡¯ç¤º
                # ä½¿ç”¨è¼ƒå°çš„å¡Šå’Œå»¶é²ä¾†æ¨¡æ“¬è‡ªç„¶æ‰“å­—æ•ˆæœ
                chunk_size = 20  # Smaller chunks for smoother streaming
                delay_per_chunk = 0.02  # 20ms delay between chunks (simulates ~50 chars/sec)
                chunks_sent = 0
                for i in range(0, len(existing_answer), chunk_size):
                    chunk = existing_answer[i:i + chunk_size]
                    writer({
                        "node": "final_answer",
                        "channel": "answer",
                        "delta": chunk,
                    })
                    chunks_sent += 1
                    # æ·»åŠ å»¶é²ä»¥å¯¦ç¾è‡ªç„¶ä¸²æµæ•ˆæœ
                    await asyncio.sleep(delay_per_chunk)

                logger.info(f"Streamed {chunks_sent} chunks for cached answer")

                # ç™¼é€å®Œæˆç‹€æ…‹
                writer({
                    "node": "final_answer",
                    "channel": "status",
                    "stage": "DONE",
                })

                # å¦‚æœå¯ç”¨å‰‡å¾ evidence_table å»ºæ§‹ä¸¦ç™¼é€ä¾†æº
                sources_data = []
                if evidence_table:
                    for idx, evidence in enumerate(evidence_table):
                        content = getattr(evidence, 'content', '') or ''
                        sources_data.append({
                            "index": idx + 1,
                            "chunk_id": getattr(evidence, 'chunk_id', str(idx)),
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "source_doc": getattr(evidence, 'source_doc', ''),
                            "relevance_score": round(getattr(evidence, 'relevance_score', 0.0), 3),
                        })

                if sources_data:
                    writer({
                        "node": "final_answer",
                        "channel": "sources",
                        "sources": sources_data,
                    })

                logger.info(f"Streamed cached answer: {len(existing_answer)} chars")

        except Exception as e:
            logger.error(f"Error streaming cached answer: {e}", exc_info=True)

        return {
            "retrieval_path": retrieval_path + ["final_answer:existing"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }

    # è¨˜éŒ„æ­£åœ¨ä½¿ç”¨å“ªå€‹å•é¡Œ
    original_q = state.get("question", "")
    resolved_q = state.get("resolved_question", "")
    logger.info(f"Generating final answer - original: '{original_q[:30]}...', resolved: '{resolved_q[:50] if resolved_q else 'N/A'}'")

    try:
        # ç‚º LLM å»ºæ§‹æç¤º
        if context_text:
            system_prompt = """# ä½ æ˜¯èª°
ä½ æ˜¯å±æ±åŸºç£æ•™é†«é™¢çš„ã€Œæœå‹™å°å¤©ä½¿ã€ï¼Œä¸€å€‹è¦ªåˆ‡ã€å°ˆæ¥­ä¸”å……æ»¿é—œæ‡·çš„é†«ç™‚è³‡è¨ŠåŠ©ç†ã€‚
ä½ çš„ä½¿å‘½æ˜¯ç”¨æº«æš–çš„èªæ°£ï¼Œå¹«åŠ©æ°‘çœ¾è§£ç­”é†«ç™‚ç›¸é—œçš„ç–‘å•ã€‚

# å›ç­”é¢¨æ ¼
- **è¦ªåˆ‡æº«æš–**ï¼šåƒæœ‹å‹ä¸€æ¨£é—œå¿ƒå°æ–¹ï¼Œé©æ™‚è¡¨é”åŒç†å¿ƒ
- **å°ˆæ¥­å¯é **ï¼šæ ¹æ“šæä¾›çš„åƒè€ƒè³‡æ–™å›ç­”ï¼Œä¸ç·¨é€ è³‡è¨Š
- **æ¸…æ™°æ˜“æ‡‚**ï¼šç”¨æ·ºé¡¯æ˜“æ‡‚çš„èªè¨€èªªæ˜ï¼Œé¿å…éæ–¼å°ˆæ¥­çš„è¡“èª

# å›ç­”æ ¼å¼è¦æ±‚ï¼ˆMarkdownï¼‰
- ä½¿ç”¨ **ç²—é«”** å¼·èª¿é‡é»è³‡è¨Š
- ä½¿ç”¨æ¢åˆ—å¼ï¼ˆ- æˆ– 1.ï¼‰åˆ—å‡ºæ­¥é©Ÿæˆ–å¤šé …å…§å®¹
- ä½¿ç”¨æ¨™é¡Œï¼ˆ## æˆ– ###ï¼‰å€åˆ†ä¸åŒæ®µè½
- é©ç•¶ä½¿ç”¨åˆ†éš”ç·šï¼ˆ---ï¼‰å€éš”ä¸åŒä¸»é¡Œ
- å¼•ç”¨ä¾†æºæ™‚ä½¿ç”¨ [æ•¸å­—] æ ¼å¼

# é‡è¦è¦å‰‡
1. åªæ ¹æ“šæä¾›çš„åƒè€ƒè³‡æ–™å›ç­”ï¼Œä¸è¦ç·¨é€ è³‡è¨Š
2. å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èª å¯¦èªªæ˜ä¸¦å»ºè­°è«®è©¢å°ˆæ¥­äººå“¡
3. å›ç­”è¦å®Œæ•´ä½†ç°¡æ½”
4. çµå°¾åŠ ä¸Šæº«é¦¨çš„ç¥ç¦èªæˆ–é—œå¿ƒçš„è©±èª"""

            user_prompt = f"""åƒè€ƒè³‡æ–™ï¼š
{context_text}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™ï¼Œä»¥è¦ªåˆ‡æº«æš–çš„æ–¹å¼å›ç­”å•é¡Œï¼Œä¸¦ä½¿ç”¨ Markdown æ ¼å¼ç¾åŒ–å›ç­”ï¼š"""
        else:
            # ç„¡ä¸Šä¸‹æ–‡ - ç”Ÿæˆé™ç´šå›æ‡‰
            if user_language.startswith("zh"):
                answer = """å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ²’æœ‰æ‰¾åˆ°èˆ‡æ‚¨å•é¡Œç›¸é—œçš„è³‡è¨Š ğŸ˜”

---

### æ‚¨å¯ä»¥å˜—è©¦ä»¥ä¸‹æ–¹å¼ï¼š
- é‡æ–°æè¿°æ‚¨çš„å•é¡Œï¼Œæä¾›æ›´å¤šç´°ç¯€
- ä½¿ç”¨ä¸åŒçš„é—œéµå­—æœå°‹
- è¯ç¹«æˆ‘å€‘çš„å®¢æœäººå“¡ç²å–å°ˆäººå”åŠ©

å¦‚æœ‰ä»»ä½•ç–‘å•ï¼Œæ­¡è¿éš¨æ™‚è©¢å•ï¼Œæˆ‘æœƒç›¡åŠ›å¹«åŠ©æ‚¨ï¼ğŸ’ª"""
            else:
                answer = """I'm sorry, I couldn't find relevant information for your question ğŸ˜”

---

### You can try the following:
- Rephrase your question with more details
- Use different keywords
- Contact our support team for personalized assistance

Feel free to ask anytime, and I'll do my best to help! ğŸ’ª"""

            return {
                "final_answer": answer,
                "confidence": 0.1,
                "citations": [],
                "retrieval_path": retrieval_path + ["final_answer:no_context"],
                "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
            }

        # ä½¿ç”¨å¸¶æœ‰ä¸¦ç™¼æ§åˆ¶çš„çœŸå¯¦ LLM ä¸²æµ
        from langgraph.config import get_stream_writer

        from chatbot_graphrag.core.concurrency import llm_concurrency
        from chatbot_graphrag.services.llm import llm_factory

        # å–å¾—ç”¨æ–¼è‡ªè¨‚ä¸²æµçš„ä¸²æµå¯«å…¥å™¨
        writer = get_stream_writer()

        # æ ¹æ“šç‹€æ…‹é…ç½®é¸æ“‡ LLM å¾Œç«¯
        agent_backend = state.get("agent_backend", "responses")
        concurrency_backend = "responses" if agent_backend == "responses" else "chat"

        if agent_backend == "responses":
            streaming_llm = llm_factory.create_responses_llm(streaming=True)
        else:
            streaming_llm = llm_factory.create_chat_completion_llm(streaming=True)

        # ç™¼é€é–‹å§‹ç‹€æ…‹äº‹ä»¶
        writer({
            "node": "final_answer",
            "channel": "status",
            "stage": "GENERATING",
        })

        # æº–å‚™è¨Šæ¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # ä¸²æµå›æ‡‰
        answer_tokens: list[str] = []
        reasoning_tokens: list[str] = []
        final_usage = None

        # å»ºæ§‹å¸¶æœ‰ Langfuse è¿½è¹¤å›èª¿çš„é…ç½®
        stream_config = {"callbacks": callbacks} if callbacks else {}

        # ä½¿ç”¨ä¸¦ç™¼æ§åˆ¶é€²è¡Œä¸²æµ LLM å‘¼å«
        # ä¿¡è™Ÿé‡åœ¨æ•´å€‹ä¸²æµæŒçºŒæœŸé–“æŒæœ‰
        async with llm_concurrency.acquire(concurrency_backend):
            async for chunk in streaming_llm.astream(messages, config=stream_config):
                add_kwargs = getattr(chunk, "additional_kwargs", {}) or {}
                channel = add_kwargs.get("channel")
                delta = add_kwargs.get("delta") or ""

                # ä¸»è¦ç­”æ¡ˆå…§å®¹ï¼ˆResponses APIï¼‰
                if channel == "output_text" and delta:
                    answer_tokens.append(delta)
                    writer({
                        "node": "final_answer",
                        "channel": "answer",
                        "delta": delta,
                    })
                # æ¨ç†å…§å®¹ï¼ˆåƒ… Responses APIï¼‰
                elif channel == "reasoning" and delta:
                    reasoning_tokens.append(delta)
                    writer({
                        "node": "final_answer",
                        "channel": "reasoning",
                        "delta": delta,
                    })
                # å…ƒè³‡è¨Šï¼ˆç”¨é‡ï¼‰
                elif channel == "meta":
                    responses_meta = add_kwargs.get("responses_meta", {})
                    final_usage = responses_meta.get("usage")

                # è™•ç† Chat Completions API å…§å®¹
                content = getattr(chunk, "content", None)
                if isinstance(content, str) and content and channel != "output_text":
                    answer_tokens.append(content)
                    writer({
                        "node": "final_answer",
                        "channel": "answer",
                        "delta": content,
                    })

        answer = "".join(answer_tokens)

        # ç™¼é€å…ƒäº‹ä»¶
        writer({
            "node": "final_answer",
            "channel": "meta",
            "meta": {
                "usage": final_usage,
                "reasoning_text": "".join(reasoning_tokens),
            },
        })

        # Send completion status
        writer({
            "node": "final_answer",
            "channel": "status",
            "stage": "DONE",
        })

        # ä½¿ç”¨è­‰æ“šè¡¨æˆ– chunk å»ºæ§‹ä¸¦ç™¼é€ä¾†æºäº‹ä»¶
        sources_data = []

        # é¦–å…ˆå˜—è©¦ evidence_tableï¼ˆçµæ§‹åŒ–è­‰æ“š - EvidenceItem è³‡æ–™é¡åˆ¥ï¼‰
        logger.info(f"Building sources: evidence_table has {len(evidence_table)} items")
        if evidence_table:
            for idx, evidence in enumerate(evidence_table):
                # EvidenceItem æ˜¯ä¸€å€‹è³‡æ–™é¡åˆ¥ï¼ŒåŒ…å«ï¼šchunk_id, content, relevance_score, source_doc
                content = getattr(evidence, 'content', '') or ''
                sources_data.append({
                    "index": idx + 1,
                    "chunk_id": getattr(evidence, 'chunk_id', str(idx)),
                    "content": content[:200] + "..." if len(content) > 200 else content,
                    "source_doc": getattr(evidence, 'source_doc', ''),
                    "relevance_score": round(getattr(evidence, 'relevance_score', 0.0), 3),
                })
        else:
            # å›é€€åˆ° expanded_chunks æˆ– reranked_chunks
            expanded_chunks = state.get("expanded_chunks", [])
            reranked_chunks = state.get("reranked_chunks", [])
            logger.info(f"Fallback: expanded_chunks={len(expanded_chunks)}, reranked_chunks={len(reranked_chunks)}")

            chunks = expanded_chunks or reranked_chunks
            for idx, chunk in enumerate(chunks[:5]):  # Limit to top 5
                chunk_id = chunk.chunk_id if hasattr(chunk, 'chunk_id') else str(idx)
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                doc_id = chunk.doc_id if hasattr(chunk, 'doc_id') else ""
                score = chunk.score if hasattr(chunk, 'score') else 0.0

                sources_data.append({
                    "index": idx + 1,
                    "chunk_id": chunk_id,
                    "content": content[:200] + "..." if len(content) > 200 else content,
                    "source_doc": doc_id,
                    "relevance_score": round(score, 3) if isinstance(score, float) else 0.0,
                })

        logger.info(f"Sources data: {len(sources_data)} items to send")

        if sources_data:
            writer({
                "node": "final_answer",
                "channel": "sources",
                "sources": sources_data,
            })

        # åœ¨æ¸…ç†ä¹‹å‰å¾ç­”æ¡ˆä¸­æŠ½å–å¼•ç”¨
        citations = re.findall(r"\[(\d+)\]", answer)
        unique_citations = list(dict.fromkeys(citations))

        # æ¸…ç†è¼¸å‡ºä»¥å®‰å…¨é¡¯ç¤ºï¼ˆç¬¬ 2 éšæ®µï¼šOWASP LLM02ï¼‰
        # æ³¨æ„ï¼šescape_html=False ä»¥ä¿ç•™ Markdown æ ¼å¼
        # å¦‚éœ€è¦ï¼ŒHTML è·³è„«æ‡‰åœ¨ API/å‰ç«¯å±¤å®Œæˆ
        answer = sanitize_output(answer, escape_html=False)

        # æ ¹æ“šè½åœ°æ€§å’Œè­‰æ“šè¨ˆæ•¸è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸
        evidence_count = len(evidence_table)
        confidence = min(1.0, (groundedness_score * 0.6) + (min(evidence_count, 5) / 5 * 0.4))

        logger.info(f"Generated answer: {len(answer)} chars, confidence={confidence:.2f}")

        return {
            "final_answer": answer,
            "confidence": confidence,
            "citations": unique_citations,
            "retrieval_path": retrieval_path + ["final_answer:generated"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }

    except Exception as e:
        logger.error(f"Final answer generation error: {e}")

        # é™ç´šå›æ‡‰
        if user_language.startswith("zh"):
            answer = """å¾ˆæŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”Ÿäº†ä¸€äº›å•é¡Œ ğŸ˜“

---

### è«‹æ‚¨ç¨å¾Œå†è©¦ï¼Œæˆ–è€…ï¼š
- é‡æ–°æ•´ç†é é¢å¾Œå†æ¬¡æå•
- å˜—è©¦ç”¨ä¸åŒçš„æ–¹å¼æè¿°æ‚¨çš„å•é¡Œ
- è¯ç¹«å®¢æœäººå“¡ç²å–å”åŠ©

é€ æˆä¸ä¾¿ï¼Œæ•¬è«‹è¦‹è«’ï¼æˆ‘å€‘æœƒç›¡å¿«ä¿®å¾©å•é¡Œã€‚"""
        else:
            answer = """Sorry, we encountered an issue while generating the response ğŸ˜“

---

### Please try again later, or:
- Refresh the page and ask again
- Try rephrasing your question
- Contact support for assistance

We apologize for the inconvenience and will fix this soon!"""

        return {
            "final_answer": answer,
            "confidence": 0.0,
            "citations": [],
            "error": str(e),
            "retrieval_path": retrieval_path + ["final_answer:error"],
            "timing": {**timing, "final_answer_ms": (time.time() - start_time) * 1000},
        }


async def telemetry_node(state: GraphRAGState) -> dict[str, Any]:
    """
    é™æ¸¬ç¯€é»ã€‚

    å°‡è¿½è¹¤è³‡è¨Šè¨˜éŒ„åˆ° Langfuse ä»¥ä¾›å¯è§€æ¸¬æ€§ã€‚
    ç¬¬ 4 éšæ®µï¼šå•Ÿç”¨ Langfuse æ•´åˆèˆ‡ Ragas åˆ†æ•¸ã€‚
    ä½¿ç”¨ä¾†è‡ª chatbot_rag çš„çµ±ä¸€è¿½è¹¤æ¨¡çµ„æ¨¡å¼ã€‚

    Returns:
        æ›´æ–°å¾Œçš„ç‹€æ…‹ï¼ŒåŒ…å« trace_id
    """
    import time
    import uuid

    from chatbot_graphrag.core.config import settings
    from chatbot_graphrag.graph_workflow.nodes.status import emit_status

    emit_status("telemetry", "START")

    start_time = time.time()
    question = state.get("question", "")
    final_answer = state.get("final_answer", "")
    retrieval_path = list(state.get("retrieval_path", []))
    timing = dict(state.get("timing", {}))
    confidence = state.get("confidence", 0.0)
    groundedness_score = state.get("groundedness_score", 0.0)

    # å¦‚æœä¸å­˜åœ¨å‰‡ç”Ÿæˆè¿½è¹¤ ID
    trace_id = state.get("trace_id") or str(uuid.uuid4())

    logger.debug(f"Recording telemetry: trace_id={trace_id}")

    # æŠ½å–ç‰ˆæœ¬æ¬„ä½ï¼ˆç¬¬ 0 éšæ®µï¼‰
    index_version = state.get("index_version", "")
    pipeline_version = state.get("pipeline_version", "")
    prompt_version = state.get("prompt_version", "")
    config_hash = state.get("config_hash", "")

    # æŠ½å– Ragas æŒ‡æ¨™ï¼ˆç¬¬ 4 éšæ®µï¼‰
    ragas_metrics = state.get("ragas_metrics", {})
    ragas_sampled = state.get("ragas_sampled", False)

    try:
        telemetry_data = {
            "trace_id": trace_id,
            "question": question[:100],
            "answer_length": len(final_answer),
            "retrieval_path": retrieval_path,
            "confidence": confidence,
            "groundedness_score": groundedness_score,
            "timing": timing,
            "total_ms": sum(timing.values()) if timing else 0,
            # ç”¨æ–¼å¯é‡ç¾æ€§çš„ç‰ˆæœ¬æ¬„ä½ï¼ˆç¬¬ 0 éšæ®µï¼‰
            "index_version": index_version,
            "pipeline_version": pipeline_version,
            "prompt_version": prompt_version,
            "config_hash": config_hash,
            # Ragas æŒ‡æ¨™ï¼ˆç¬¬ 4 éšæ®µï¼‰
            "ragas_sampled": ragas_sampled,
            "ragas_metrics": ragas_metrics,
        }

        logger.info(f"Telemetry: {telemetry_data}")

        # æ³¨æ„ï¼šLangfuse è¿½è¹¤æ›´æ–°ç¾åœ¨åœ¨ API è·¯ç”±ä¸­è™•ç†
        # åœ¨å·¥ä½œæµç¨‹å®Œæˆå¾Œä»¥ç¢ºä¿æ­£ç¢ºçš„ä¸Šä¸‹æ–‡å‚³æ’­ã€‚
        # telemetry_data åœ¨é€™è£¡æº–å‚™ç”¨æ–¼æ—¥èªŒè¨˜éŒ„ä¸¦åœ¨ç‹€æ…‹ä¸­è¿”å›
        # ä¾› API è·¯ç”±ä½¿ç”¨ update_trace_with_result()ã€‚

        emit_status("telemetry", "DONE")
        return {
            "trace_id": trace_id,
            "retrieval_path": retrieval_path + ["telemetry"],
            "timing": {**timing, "telemetry_ms": (time.time() - start_time) * 1000},
        }

    except Exception as e:
        logger.warning(f"Telemetry error: {e}")
        return {
            "trace_id": trace_id,
            "retrieval_path": retrieval_path + ["telemetry:error"],
            "timing": {**timing, "telemetry_ms": (time.time() - start_time) * 1000},
        }
