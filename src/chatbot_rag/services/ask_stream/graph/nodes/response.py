"""
å›ç­”ç”Ÿæˆç¯€é»ï¼šç”¢ç”Ÿæœ€çµ‚å›ç­”ä¸¦é€é SSE ä¸²æµè¼¸å‡ºã€‚

åŠŸèƒ½ï¼š
- æ ¹æ“š intent å’Œ context çµ„åˆ LLM prompt
- SSE ä¸²æµè¼¸å‡ºå›ç­”å…§å®¹ï¼ˆanswer äº‹ä»¶ï¼‰
- ç”¢ç”Ÿå°è©±æ‘˜è¦ï¼ˆä¾›ä¸‹ä¸€è¼ªå°è©±ä½¿ç”¨ï¼‰
- è™•ç† fallback å›æ‡‰ï¼ˆç•¶ LLM å¤±æ•—æˆ–ç„¡çµæœæ™‚ï¼‰

SSE äº‹ä»¶ï¼š
- answer: delta ä¸²æµå…§å®¹
- meta: usage/duration çµ±è¨ˆ

å‰ç½®ç¯€é»ï¼šresult_evaluator / followup_transform / intent_analyzer
å¾ŒçºŒç¯€é»ï¼šcache_store
"""

from typing import Any, Callable, Optional, List, Dict, cast
import time

from langchain_core.messages import BaseMessage, AIMessage, SystemMessage, HumanMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.config import get_stream_writer
from loguru import logger

from chatbot_rag.core.concurrency import llm_concurrency, with_llm_semaphore
from chatbot_rag.llm import State, create_chat_completion_llm, create_responses_llm
from chatbot_rag.services.language_utils import (
    normalize_usage_payload,
    extract_usage_from_llm_output,
)
from chatbot_rag.services.prompt_service import PromptService, PromptNames, DEFAULT_PROMPTS
from ...types import StateDict, generation_inputs_from_state
from ...constants import (
    AskStreamStages,
    RESPONSE_HISTORY_LIMIT,
    CONVERSATION_SUMMARY_MAX_CHARS,
    SUPPORT_SCOPE_TEXT,
)
from ...events import emit_node_event, emit_llm_meta_event
from ...utils import (
    message_to_text,
    select_conversation_history,
    fallback_conversation_summary,
)


# Fallback å›æ‡‰æ¨¡æ¿ï¼ˆæŒ‰ intent åˆ†é¡ï¼‰- å±æ±åŸºç£æ•™é†«é™¢é¢¨æ ¼
FALLBACK_RESPONSES = {
    "simple_faq": """æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡Œæˆ‘ç›®å‰æŸ¥ä¸åˆ°é‚£éº¼ç´°çš„è³‡æ–™ï¼Œ
æœ‰å¯èƒ½æ˜¯è³‡è¨Šé‚„æœªå®Œå…¨ä¸Šç·šï¼Œä¹Ÿå¯èƒ½æ‚¨çš„å•é¡Œéœ€è¦æ›´å°ˆæ¥­çš„å–®ä½èªªæ˜ï½
å»ºè­°æ‚¨å‰å¾€å±åŸºå®˜ç¶²æŸ¥è©¢ï¼š<a href="https://www.ptch.org.tw/index.php/index" target="_blank">å±æ±åŸºç£æ•™é†«é™¢å®˜ç¶²</a>
æˆ–è‡´é›»å®¢æœå°ˆç·šï¼šâ˜ï¸ 08-7368686

æ‚¨çœŸçš„å¾ˆé—œå¿ƒå¥åº·è€¶ï¼è¬è¬æ‚¨çš„è€å¿ƒï¼Œä¹Ÿæ­¡è¿éš¨æ™‚å†å›ä¾†å•æˆ‘å”·ï¼""",
    "symptom_inquiry": """æŠ±æ­‰ï¼Œé—œæ–¼æ‚¨æè¿°çš„ç—‡ç‹€ï¼Œæˆ‘ç›®å‰æŸ¥ä¸åˆ°é‚£éº¼ç´°çš„è³‡æ–™ï½
å»ºè­°æ‚¨å¯ä»¥å…ˆåƒè€ƒæˆ‘å€‘çš„é–€è¨ºæ™‚åˆ»è¡¨ï¼š<a href="https://www.ptch.org.tw/ebooks/" target="_blank">é–€è¨ºæ™‚åˆ»è¡¨</a>
æˆ–ç›´æ¥è‡´é›»å®¢æœå°ˆç·šï¼šâ˜ï¸ 08-7368686 è©¢å•é©åˆçš„ç§‘åˆ¥

èº«é«”å¥åº·æœ€é‡è¦ï¼Œå¸Œæœ›æ‚¨æ—©æ—¥åº·å¾©ï¼""",
    "privacy_inquiry": """éå¸¸æŠ±æ­‰ï¼Œæ‚¨è©¢å•çš„å•é¡Œæ¶‰åŠåˆ°å€‹äººè³‡æ–™çš„éƒ¨åˆ†ï¼Œ
åŸºæ–¼å€‹è³‡ä¿è­·çš„è¦å®šï¼Œé€™é¡è³‡è¨Šç„¡æ³•åœ¨æ­¤æŸ¥è©¢å–”ï½
å¦‚æœéœ€è¦æŸ¥è©¢æ‚¨è‡ªå·±çš„å°±é†«ç´€éŒ„ï¼Œå»ºè­°æ‚¨ï¼š
1. è¦ªè‡ªè‡³é†«é™¢çš„æœå‹™å°æ´½è©¢
2. æˆ–è‡´é›»å®¢æœå°ˆç·šï¼šâ˜ï¸ 08-7368686

æ„Ÿè¬æ‚¨çš„ç†è§£ï¼Œä¹Ÿç¥æ‚¨å¥åº·å¹³å®‰ï¼""",
    "conversation_followup": """æŠ±æ­‰ï¼Œæˆ‘å‰›æ‰çš„å›ç­”å¯èƒ½æ²’æœ‰å®Œå…¨æ»¿è¶³æ‚¨çš„éœ€æ±‚ï½
èƒ½å¦è«‹æ‚¨å†èªªæ˜ä¸€ä¸‹æƒ³äº†è§£çš„éƒ¨åˆ†å‘¢ï¼Ÿæˆ‘æœƒç›¡åŠ›å¹«æ‚¨è§£ç­”ï¼

ç¥æ‚¨ä¸€åˆ‡é †åˆ©ï¼Œæœ‰éœ€è¦æˆ‘ä¸€ç›´éƒ½åœ¨é€™è£¡å–”ï½""",
    "out_of_scope": """é€™å€‹å•é¡Œæˆ‘å¯èƒ½ä¸æ˜¯å¾ˆç†è§£ï¼Œä¸éæ²’é—œä¿‚ï½
å¦‚æœæ‚¨å°å¥åº·æˆ–å°±é†«æœ‰ä»»ä½•éœ€è¦ï¼Œæˆ‘éƒ½å¾ˆé¡˜æ„å¹«å¿™å–”ï¼
ä¾‹å¦‚ï¼šé–€è¨ºæ™‚é–“ã€æ›è™Ÿæµç¨‹ã€ç§‘åˆ¥è«®è©¢ç­‰å•é¡Œï¼Œéƒ½å¯ä»¥å•æˆ‘ï½

ç¥æ‚¨å¥åº·å¹³å®‰ï¼Œæœ‰éœ€è¦éš¨æ™‚æ‰¾æˆ‘å–”ï¼""",
    "default": """æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚é‡åˆ°ä¸€äº›ç‹€æ³ï¼Œç„¡æ³•å›ç­”æ‚¨çš„å•é¡Œï½
å»ºè­°æ‚¨å¯ä»¥ï¼š
1. ç¨å¾Œå†è©¦ä¸€æ¬¡
2. å‰å¾€å±åŸºå®˜ç¶²æŸ¥è©¢ï¼š<a href="https://www.ptch.org.tw/index.php/index" target="_blank">å±æ±åŸºç£æ•™é†«é™¢å®˜ç¶²</a>
3. è‡´é›»å®¢æœå°ˆç·šï¼šâ˜ï¸ 08-7368686

æ„Ÿè¬æ‚¨çš„è€å¿ƒï¼Œç¥æ‚¨å¥åº·å¹³å®‰ï¼""",
}


def _generate_fallback_response(
    *,
    intent: str,
    user_language: str,
    error: Optional[Exception] = None,
    include_error_hint: bool = False,
) -> str:
    """
    ç”¢ç”Ÿ fallback å›æ‡‰å…§å®¹ã€‚

    Args:
        intent: ä»»å‹™æ„åœ–ï¼ˆç”¨æ–¼é¸æ“‡å›æ‡‰æ¨¡æ¿ï¼‰
        user_language: ä½¿ç”¨è€…èªè¨€
        error: è§¸ç™¼ fallback çš„éŒ¯èª¤ï¼ˆå¯é¸ï¼‰
        include_error_hint: æ˜¯å¦åŒ…å«éŒ¯èª¤æç¤ºï¼ˆåƒ…ç”¨æ–¼ debugï¼‰

    Returns:
        Fallback å›æ‡‰æ–‡å­—ï¼ˆå±æ±åŸºç£æ•™é†«é™¢é¢¨æ ¼ï¼‰
    """
    base_response = FALLBACK_RESPONSES.get(intent, FALLBACK_RESPONSES["default"])

    # å±æ±åŸºç£æ•™é†«é™¢ï¼šå¼·åˆ¶ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¸æä¾›è‹±æ–‡ç‰ˆæœ¬
    # å³ä½¿ä½¿ç”¨è€…ä½¿ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼ˆç¬¦åˆå®¢æˆ¶è¦æ±‚ï¼šç¦æ­¢ç°¡é«”ä¸­æ–‡ï¼‰

    if include_error_hint and error:
        error_type = type(error).__name__
        base_response += f"\n\nï¼ˆæŠ€è¡“åƒè€ƒï¼š{error_type}ï¼‰"

    return base_response


async def _summarize_conversation_history(
    *,
    prev_summary: str,
    latest_user: str,
    latest_answer: str,
    base_llm_params: dict,
    prompt_service: Optional[PromptService] = None,
) -> tuple[str, Optional[dict[str, Any]], Optional[float]]:
    """
    é€éå°å‹ LLM ç”¢ç”Ÿå°è©±æ‘˜è¦ï¼Œæä¾›é•·æœŸè¨˜æ†¶ï¼›å¤±æ•—æ™‚é€€å›ç°¡æ˜“ä¸²æ¥ï¼ˆéåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚
    """
    from chatbot_rag.core.config import settings

    if not latest_user and not latest_answer:
        return prev_summary, None, None

    summarizer_model = base_llm_params.get("model") or settings.chat_model
    summary_llm = create_responses_llm(
        streaming=False,
        reasoning_effort=base_llm_params.get("reasoning_effort", "low"),
        reasoning_summary=None,
        model=summarizer_model,
    )

    # å¾ Langfuse ç²å– summarizer promptï¼ˆfallback ä½¿ç”¨ DEFAULT_PROMPTSï¼‰
    fallback_prompt = DEFAULT_PROMPTS[PromptNames.CONVERSATION_SUMMARIZER]["prompt"]
    if prompt_service:
        try:
            system_prompt, _ = prompt_service.get_text_prompt(
                PromptNames.CONVERSATION_SUMMARIZER
            )
        except Exception as exc:
            logger.warning(f"[SUMMARY] Failed to fetch prompt from Langfuse: {exc}")
            system_prompt = fallback_prompt
    else:
        system_prompt = fallback_prompt
    content_sections = [
        f"ã€æ—¢æœ‰æ‘˜è¦ã€‘\n{prev_summary.strip() or 'ï¼ˆç„¡ï¼‰'}",
        f"ã€æœ€æ–°å°è©±ã€‘\nä½¿ç”¨è€…ï¼š{latest_user.strip() or 'ï¼ˆç„¡ï¼‰'}\nåŠ©ç†ï¼š{latest_answer.strip() or 'ï¼ˆç„¡ï¼‰'}",
        "è«‹ç”¢å‡ºæ–°çš„ç¶œåˆæ‘˜è¦ï¼Œè‹¥è³‡è¨Šé‡è¤‡å¯ç•¥ï¼Œç¶­æŒç¹é«”ä¸­æ–‡ã€‚",
    ]

    start = time.monotonic()
    try:
        raw = await with_llm_semaphore(
            lambda: summary_llm.ainvoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content="\n\n".join(content_sections)),
                ],
            ),
            backend="responses",
        )
        duration_ms = (time.monotonic() - start) * 1000.0
        usage = extract_usage_from_llm_output(raw)
        summary = message_to_text(raw).strip()
        if not summary:
            summary = fallback_conversation_summary(
                prev_summary, latest_user, latest_answer
            )
        if len(summary) > CONVERSATION_SUMMARY_MAX_CHARS:
            summary = summary[:CONVERSATION_SUMMARY_MAX_CHARS]
        return summary, usage, duration_ms
    except Exception as exc:  # noqa: BLE001 pylint: disable=broad-exception-caught
        logger.warning("[SUMMARY] conversation summarization failed: %s", exc)
        return (
            fallback_conversation_summary(prev_summary, latest_user, latest_answer),
            None,
            (time.monotonic() - start) * 1000.0,
        )


def _build_task_analysis_section(
    *,
    task_type: str,
    intent: str,
    used_tools: Optional[List[str]] = None,
    context_text: str = "",
) -> str:
    """
    å»ºç«‹ä»»å‹™åˆ†æå€æ®µï¼ˆNon-followup å°ˆç”¨ï¼‰ã€‚

    é‡å°å°æ¨¡å‹å„ªåŒ–ï¼šç²¾ç°¡ã€çµæ§‹åŒ–ã€é—œéµç´„æŸå‰ç½®ã€‚

    Args:
        task_type: ä»»å‹™é¡å‹
        intent: ä½¿ç”¨è€…æ„åœ–
        used_tools: å·²ä½¿ç”¨å·¥å…·
        context_text: çŸ¥è­˜åº«å…§å®¹

    Returns:
        str: ä»»å‹™åˆ†æå€æ®µæ–‡å­—ï¼ˆMarkdown æ ¼å¼ï¼‰
    """
    has_context = bool(context_text and context_text.strip())

    # ç²¾ç°¡ç‰ˆä»»å‹™è³‡è¨Šï¼ˆå°æ¨¡å‹å‹å¥½ï¼‰
    lines = [
        "# ä»»å‹™",
        f"æ„åœ–ï¼š{intent}",
    ]

    if has_context:
        # æœ‰çŸ¥è­˜åº«å…§å®¹ï¼šå¼·èª¿ã€Œåªç”¨çŸ¥è­˜åº«ã€
        lines.extend([
            "",
            "# âš ï¸ å›ç­”è¦å‰‡",
            "1. **åªç”¨**ä¸‹æ–¹ã€ŒçŸ¥è­˜åº«å…§å®¹ã€å›ç­”",
            "2. çŸ¥è­˜åº«æ²’æœ‰çš„ â†’ èªªã€ŒæŸ¥ä¸åˆ°ã€ï¼Œå¼•å°è‡´é›»å®¢æœ",
            "3. **ç¦æ­¢**ç·¨é€ é†«å¸«ã€æ™‚é–“ã€ç§‘åˆ¥",
            "",
            "ç¯„ä¾‹ï¼šçŸ¥è­˜åº«åªæœ‰ã€Œç‹é†«å¸«ã€æé†«å¸«ã€",
            "- âœ…ã€Œæœ‰ç‹é†«å¸«ã€æé†«å¸«ã€",
            "- âŒã€Œæœ‰ç‹é†«å¸«ã€æé†«å¸«ã€å¼µé†«å¸«ç­‰ã€ï¼ˆå¼µé†«å¸«æ˜¯ç·¨é€ çš„ï¼‰",
        ])
    else:
        # ç„¡çŸ¥è­˜åº«å…§å®¹ï¼šå¼•å°ä½¿ç”¨æ¨™æº–å›æ‡‰
        lines.extend([
            "",
            "# âš ï¸ ç‹€æ…‹ï¼šæŸ¥ç„¡è³‡æ–™",
            "è«‹ä½¿ç”¨æ¨™æº–å›æ‡‰ï¼š",
            "ã€Œç›®å‰æŸ¥ä¸åˆ°ç›¸é—œè³‡æ–™ï¼Œå»ºè­°è‡´é›»å®¢æœ 08-7368686 æˆ–æŸ¥è©¢å®˜ç¶²ã€",
        ])

    return "\n".join(lines)


def _build_context_section(context_text: str) -> str:
    """
    å»ºç«‹çŸ¥è­˜åº«å…§å®¹å€æ®µã€‚

    Args:
        context_text: çŸ¥è­˜åº«å…§å®¹

    Returns:
        str: çŸ¥è­˜åº«å€æ®µæ–‡å­—ï¼ˆMarkdown æ ¼å¼ï¼Œç©ºå­—ä¸²è¡¨ç¤ºç„¡å…§å®¹ï¼‰
    """
    if not context_text:
        return ""
    lines = [
        "# çŸ¥è­˜åº«å…§å®¹",
        "",
        context_text,
    ]
    return "\n".join(lines)


def _build_prev_answer_section(prev_answer: str) -> str:
    """
    å»ºç«‹ä¸Šä¸€è¼ªå›ç­”å€æ®µï¼ˆFollowup å°ˆç”¨ï¼‰ã€‚

    Args:
        prev_answer: ä¸Šä¸€è¼ªå›ç­”

    Returns:
        str: ä¸Šä¸€è¼ªå›ç­”å€æ®µæ–‡å­—ï¼ˆMarkdown æ ¼å¼ï¼‰
    """
    if not prev_answer:
        return ""
    lines = [
        "# ä¸Šä¸€è¼ªå›ç­”",
        "",
        "> ä»¥ä¸‹æ˜¯ä½ ä¸Šä¸€è¼ªçš„å›ç­”ï¼Œè«‹åŸºæ–¼æ­¤å…§å®¹é€²è¡Œå¾ŒçºŒè™•ç†ã€‚",
        "",
        "---",
        "",
        prev_answer,
    ]
    return "\n".join(lines)


def _build_conversation_summary_section(conversation_summary: str) -> str:
    """
    å»ºç«‹å°è©±æ‘˜è¦å€æ®µã€‚

    Args:
        conversation_summary: å°è©±æ‘˜è¦

    Returns:
        str: å°è©±æ‘˜è¦å€æ®µæ–‡å­—ï¼ˆMarkdown æ ¼å¼ï¼Œç©ºå­—ä¸²è¡¨ç¤ºç„¡å…§å®¹ï¼‰
    """
    if not conversation_summary:
        return ""
    lines = [
        "# å°è©±æ‘˜è¦ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰",
        "",
        "> ğŸ’¡ ä»¥ä¸‹æ˜¯å…ˆå‰å°è©±çš„æ‘˜è¦ï¼Œä¾›ä½ åƒè€ƒä¸Šä¸‹æ–‡ï¼Œè«‹å‹¿é€å­—è¼¸å‡ºã€‚",
        "",
        conversation_summary,
    ]
    return "\n".join(lines)


def _get_role_definition(
    user_language: str,
    prompt_service: Optional[PromptService] = None,
) -> str:
    """
    å–å¾—å…±ç”¨è§’è‰²å®šç¾© promptã€‚

    Args:
        user_language: ä½¿ç”¨è€…èªè¨€
        prompt_service: Prompt æœå‹™

    Returns:
        str: ç·¨è­¯å¾Œçš„è§’è‰²å®šç¾©
    """
    lang_instruction_name = PromptNames.get_language_instruction_name(user_language)
    if prompt_service:
        try:
            lang_instruction, _ = prompt_service.get_text_prompt(lang_instruction_name)
        except Exception:
            lang_instruction = DEFAULT_PROMPTS[lang_instruction_name]["prompt"]
    else:
        lang_instruction = DEFAULT_PROMPTS[lang_instruction_name]["prompt"]

    # å–å¾—è§’è‰²å®šç¾©
    if prompt_service:
        try:
            role_def, _ = prompt_service.get_text_prompt(
                PromptNames.ROLE_DEFINITION,
                language_instruction=lang_instruction,
            )
            return role_def
        except Exception:
            pass

    # Fallback
    role_template = DEFAULT_PROMPTS[PromptNames.ROLE_DEFINITION]["prompt"]
    return role_template.replace("{{language_instruction}}", lang_instruction)


def _get_intent_instruction(intent: str) -> tuple[str, str]:
    """
    æ ¹æ“š intent å–å¾—å°æ‡‰çš„æè¿°å’ŒæŒ‡ä»¤ã€‚

    Args:
        intent: ä½¿ç”¨è€…æ„åœ–

    Returns:
        tuple: (intent_description, intent_instruction)
    """
    intent_configs = {
        "privacy_inquiry": (
            "å©‰æ‹’å€‹è³‡æŸ¥è©¢",
            """æ°‘çœ¾è©¢å•çš„æ˜¯å€‹äººé†«ç™‚è³‡è¨Šï¼ˆå¦‚ç—…æ­·ã€çœ‹è¨ºè¨˜éŒ„ã€è²»ç”¨æ˜ç´°ç­‰ï¼‰ã€‚

## å›æ‡‰è¦é»

1. **æ„Ÿè¬ä¸¦è¡¨é”ç†è§£**ï¼šã€Œæ„Ÿè¬æ‚¨çš„æå•ï¼ã€
2. **èªªæ˜åŸå› **ï¼šç‚ºä¿è­·éš±ç§æ¬Šç›Šï¼Œé€™äº›è³‡è¨Šéœ€é€éæ­£å¼ç®¡é“æŸ¥è©¢
3. **æä¾›æ›¿ä»£æ–¹æ¡ˆ**ï¼š
   - è¦ªè‡ªè‡³é†«é™¢æœå‹™å°ç”³è«‹
   - è‡´é›»å®¢æœå°ˆç·š **08-7368686**
4. **å¼•å°å…¶ä»–å•é¡Œ**ï¼šè©¢å•æ˜¯å¦æœ‰å…¶ä»–é—œæ–¼é†«é™¢æœå‹™çš„å•é¡Œå¯ä»¥å”åŠ©

**ç¯„ä¾‹é–‹é ­**ï¼šã€Œæ„Ÿè¬æ‚¨çš„æå•ï¼ğŸ˜Š é—œæ–¼æ‚¨è©¢å•çš„å€‹äººé†«ç™‚è³‡è¨Š...ã€""",
        ),
        "service_capability": (
            "èªªæ˜æœå‹™èƒ½åŠ›",
            """æ°‘çœ¾åœ¨è©¢å•ä½ èƒ½åšä»€éº¼ã€èƒ½ä¸èƒ½å¹«å¿™æŸä»¶äº‹ã€‚

## å›æ‡‰è¦é»

1. **è¦ªåˆ‡å›æ‡‰**ï¼šå…ˆè‚¯å®šæ°‘çœ¾çš„æå•
2. **èªªæ˜èƒ½åŠ›ç¯„åœ**ï¼š
   - âœ… **å¯ä»¥å”åŠ©**ï¼šæŸ¥è©¢é–€è¨ºæ™‚é–“ã€æ›è™Ÿæµç¨‹ã€ç§‘åˆ¥è«®è©¢ã€é†«å¸«è³‡è¨Šã€å°±é†«é ˆçŸ¥ç­‰
   - âŒ **ç„¡æ³•å”åŠ©**ï¼šç›´æ¥å¹«æ°‘çœ¾æ›è™Ÿã€æŸ¥è©¢å€‹äººç—…æ­·ã€é ç´„æ‰‹è¡“ç­‰éœ€è¦èº«ä»½é©—è­‰çš„æ“ä½œ
3. **æä¾›æ›¿ä»£æ–¹æ¡ˆ**ï¼š
   - ç·šä¸Šæ›è™Ÿï¼š[æˆ‘è¦æ›è™Ÿ](https://www.ptch.org.tw/index.php/reg_listForm01)
   - é›»è©±é ç´„ï¼šğŸ“ **08-7368686**
4. **ä¸»å‹•å¼•å°**ï¼šè©¢å•æ°‘çœ¾æƒ³äº†è§£å“ªæ–¹é¢çš„è³‡è¨Š

## ç¯„ä¾‹

è‹¥æ°‘çœ¾å•ã€Œä½ èƒ½å¹«æˆ‘æ›è™Ÿå—ï¼Ÿã€ï¼š
ã€Œæ‚¨å¥½ï¼ğŸ˜Š å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰ç„¡æ³•ç›´æ¥å¹«æ‚¨å®Œæˆæ›è™Ÿï¼Œä½†æˆ‘å¯ä»¥å”åŠ©æ‚¨äº†è§£æ›è™Ÿæµç¨‹å–”ï¼

æ‚¨å¯ä»¥é€éä»¥ä¸‹æ–¹å¼æ›è™Ÿï¼š
- ğŸ“± **ç·šä¸Šæ›è™Ÿ**ï¼š[æˆ‘è¦æ›è™Ÿ](https://www.ptch.org.tw/index.php/reg_listForm01)
- ğŸ“ **é›»è©±é ç´„**ï¼šæ’¥æ‰“ **08-7368686** å®¢æœå°ˆç·š

å¦‚æœæ‚¨æƒ³äº†è§£å“ªå€‹ç§‘åˆ¥é©åˆæ‚¨ï¼Œæˆ–æ˜¯æƒ³çŸ¥é“æŸä½é†«å¸«çš„é–€è¨ºæ™‚é–“ï¼Œéƒ½å¯ä»¥å•æˆ‘å–”ï¼âœ¨ã€""",
        ),
        "out_of_scope": (
            "å¼•å°å›é†«é™¢ç›¸é—œå•é¡Œ",
            """æ°‘çœ¾çš„å•é¡Œèˆ‡é†«é™¢æœå‹™ç„¡é—œï¼ˆå¦‚å¤©æ°£ã€æ—…éŠã€ç¨‹å¼ç­‰ï¼‰ã€‚

## å›æ‡‰è¦é»

1. **è¦ªåˆ‡å›æ‡‰**ï¼šä¸è¦è®“æ°‘çœ¾è¦ºå¾—è¢«æ‹’çµ•
2. **èªªæ˜æœå‹™ç¯„åœ**ï¼šä»‹ç´¹ä½ å¯ä»¥å”åŠ©çš„å•é¡Œé¡å‹
   - ğŸ“‹ æ›è™Ÿæµç¨‹ã€é–€è¨ºæ™‚é–“
   - ğŸ©º å„ç§‘åˆ¥æœå‹™è«®è©¢
   - ğŸ¥ å°±é†«é ˆçŸ¥ã€é™¢å…§è¨­æ–½
3. **æº«æš–é‚€è«‹**ï¼šæ­¡è¿æ°‘çœ¾è©¢å•å¥åº·/å°±é†«ç›¸é—œå•é¡Œ

**ç¯„ä¾‹é–‹é ­**ï¼šã€Œè¬è¬æ‚¨çš„æå•ï¼ğŸ˜Š æˆ‘æ˜¯å±åŸºçš„æœå‹™å°å¤©ä½¿ï¼Œå°ˆé–€å”åŠ©...ã€""",
        ),
        "greeting": (
            "å›æ‡‰æ‰“æ‹›å‘¼",
            """æ°‘çœ¾åªæ˜¯æ‰“æ‹›å‘¼æˆ–å¯’æš„ã€‚

## å›æ‡‰è¦é»

1. **ç†±æƒ…å›æ‡‰**ï¼šç”¨æº«æš–çš„æ–¹å¼æ‰“æ‹›å‘¼
2. **è‡ªæˆ‘ä»‹ç´¹**ï¼šç°¡å–®ä»‹ç´¹ä½ æ˜¯å±åŸºçš„æœå‹™å°å¤©ä½¿
3. **ä¸»å‹•è©¢å•**ï¼šè©¢å•æ°‘çœ¾ä»Šå¤©æœ‰ä»€éº¼å¯ä»¥å¹«å¿™çš„

**ç¯„ä¾‹**ï¼šã€Œæ‚¨å¥½ï¼ğŸ˜Š æˆ‘æ˜¯å±æ±åŸºç£æ•™é†«é™¢çš„æœå‹™å°å¤©ä½¿ï¼Œå¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ï½è«‹å•ä»Šå¤©æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«æ‚¨çš„å‘¢ï¼Ÿã€""",
        ),
    }

    return intent_configs.get(
        intent,
        ("å›æ‡‰æ°‘çœ¾å•é¡Œ", "è«‹ç”¨è¦ªåˆ‡çš„æ–¹å¼å›æ‡‰æ°‘çœ¾çš„å•é¡Œï¼Œå¦‚æœ‰éœ€è¦å¯å¼•å°è‡³å®¢æœå°ˆç·šã€‚"),
    )


def _build_complete_system_prompt(
    *,
    is_followup: bool,
    user_language: str,
    prompt_service: Optional[PromptService] = None,
    # Followup å°ˆç”¨åƒæ•¸
    prev_answer: str = "",
    # Non-followup å°ˆç”¨åƒæ•¸
    task_type: str = "",
    intent: str = "",
    used_tools: Optional[List[str]] = None,
    context_text: str = "",
    # å…±ç”¨åƒæ•¸
    conversation_summary: str = "",
) -> str:
    """
    å»ºç«‹å®Œæ•´çš„ç³»çµ± promptï¼Œæ ¹æ“šå ´æ™¯é¸æ“‡å°æ‡‰çš„ prompt æ¨¡æ¿ã€‚

    å ´æ™¯åˆ‡å‰²ï¼š
    1. Followupï¼ˆè¿½å•ï¼‰â†’ FOLLOWUP_SYSTEM
    2. æœ‰çŸ¥è­˜åº«å…§å®¹ â†’ RESPONSE_WITH_CONTEXT
    3. ç„¡çŸ¥è­˜åº«å…§å®¹ â†’ RESPONSE_NO_CONTEXT
    4. ç›´æ¥å›æ‡‰ï¼ˆprivacy_inquiry, out_of_scopeï¼‰â†’ RESPONSE_DIRECT

    Args:
        is_followup: æ˜¯å¦ç‚ºè¿½å•å ´æ™¯
        user_language: ä½¿ç”¨è€…èªè¨€
        prompt_service: Prompt æœå‹™
        prev_answer: ä¸Šä¸€è¼ªå›ç­”ï¼ˆfollowup ç”¨ï¼‰
        task_type: ä»»å‹™é¡å‹ï¼ˆnon-followup ç”¨ï¼‰
        intent: ä½¿ç”¨è€…æ„åœ–ï¼ˆnon-followup ç”¨ï¼‰
        used_tools: å·²ä½¿ç”¨å·¥å…·ï¼ˆnon-followup ç”¨ï¼‰
        context_text: çŸ¥è­˜åº«å…§å®¹ï¼ˆnon-followup ç”¨ï¼‰
        conversation_summary: å°è©±æ‘˜è¦ï¼ˆå…±ç”¨ï¼‰

    Returns:
        str: å®Œæ•´çš„ç³»çµ± prompt
    """
    # å–å¾—å…±ç”¨è§’è‰²å®šç¾©
    role_definition = _get_role_definition(user_language, prompt_service)

    # å–å¾—å°è©±æ‘˜è¦å€æ®µ
    conversation_summary_section = _build_conversation_summary_section(conversation_summary)

    # æ ¹æ“šå ´æ™¯é¸æ“‡ prompt
    if is_followup:
        # å ´æ™¯ï¼šè¿½å•è™•ç†
        prev_answer_section = _build_prev_answer_section(prev_answer)
        compile_vars = {
            "role_definition": role_definition,
            "prev_answer_section": prev_answer_section,
            "conversation_summary_section": conversation_summary_section,
        }
        prompt_name = PromptNames.FOLLOWUP_SYSTEM

    elif intent in ("privacy_inquiry", "out_of_scope", "greeting", "service_capability"):
        # å ´æ™¯ï¼šç›´æ¥å›æ‡‰ï¼ˆä¸éœ€è¦çŸ¥è­˜åº«ï¼‰
        intent_description, intent_instruction = _get_intent_instruction(intent)
        compile_vars = {
            "role_definition": role_definition,
            "intent_description": intent_description,
            "intent_instruction": intent_instruction,
            "conversation_summary_section": conversation_summary_section,
        }
        prompt_name = PromptNames.RESPONSE_DIRECT

    elif context_text and context_text.strip():
        # å ´æ™¯ï¼šæª¢ç´¢æˆåŠŸï¼ˆæœ‰çŸ¥è­˜åº«å…§å®¹ï¼‰
        context_section = _build_context_section(context_text)
        compile_vars = {
            "role_definition": role_definition,
            "context_section": context_section,
            "conversation_summary_section": conversation_summary_section,
        }
        prompt_name = PromptNames.RESPONSE_WITH_CONTEXT

    else:
        # å ´æ™¯ï¼šæª¢ç´¢å¤±æ•—ï¼ˆç„¡çŸ¥è­˜åº«å…§å®¹ï¼‰
        compile_vars = {
            "role_definition": role_definition,
            "conversation_summary_section": conversation_summary_section,
        }
        prompt_name = PromptNames.RESPONSE_NO_CONTEXT

    # å˜—è©¦å¾ Langfuse å–å¾—ä¸¦ç·¨è­¯
    if prompt_service:
        try:
            compiled_prompt, _ = prompt_service.get_text_prompt(
                prompt_name,
                **compile_vars,
            )
            return compiled_prompt
        except Exception as exc:
            logger.warning(f"[RESPONSE] Failed to fetch {prompt_name}: {exc}")

    # Fallback: ä½¿ç”¨ DEFAULT_PROMPTS
    if prompt_name in DEFAULT_PROMPTS:
        fallback_template = DEFAULT_PROMPTS[prompt_name]["prompt"]
    else:
        # æœ€å¾Œå‚™æ´ï¼šä½¿ç”¨èˆŠçš„ UNIFIED_AGENT_SYSTEM
        fallback_template = DEFAULT_PROMPTS[PromptNames.UNIFIED_AGENT_SYSTEM]["prompt"]

    for var_name, var_value in compile_vars.items():
        fallback_template = fallback_template.replace(f"{{{{{var_name}}}}}", var_value)

    return fallback_template


def build_response_node(
    base_llm_params: dict,
    *,
    agent_backend: str = "chat",
    prompt_service: Optional[PromptService] = None,
) -> Callable[[State], State]:
    """å›ç­”ç”¢ç”Ÿç¯€é»ï¼Œçµ±ä¸€è™•ç† streaming èˆ‡ metaã€‚"""
    from chatbot_rag.core.config import settings

    # å›æ‡‰ç”Ÿæˆçš„æº«åº¦è¨­å®šï¼š
    # - 0.5 è®“å›æ‡‰æ›´è‡ªç„¶ã€æº«æš–
    # - é˜²æ­¢å¹»è¦ºä¸»è¦é  prompt ä¸­çš„ç´„æŸï¼Œè€Œéä½æº«åº¦
    RESPONSE_TEMPERATURE = 0.5

    def _create_streaming_llm(task_type: str) -> BaseChatModel:
        """æ ¹æ“šä»»å‹™é¡å‹å‹•æ…‹å»ºç«‹ LLMã€‚"""
        task_params = settings.get_llm_params_for_task(task_type)
        # ä½¿ç”¨å›ºå®šçš„å›æ‡‰æº«åº¦ï¼Œè®“å›æ‡‰æ›´è‡ªç„¶
        temperature = task_params.get("temperature", RESPONSE_TEMPERATURE)
        reasoning_effort = str(task_params.get("reasoning_effort", "medium"))

        if agent_backend == "chat":
            return create_chat_completion_llm(
                streaming=True,
                model=base_llm_params.get("model"),
                temperature=float(temperature),
            )
        else:
            return cast(
                BaseChatModel,
                create_responses_llm(
                    streaming=True,
                    reasoning_effort=reasoning_effort,
                    reasoning_summary=base_llm_params.get("reasoning_summary", "auto"),
                    model=base_llm_params.get("model"),
                    temperature=float(temperature),
                ),
            )

    async def response_node(state: State) -> State:
        writer = get_stream_writer()
        state_dict = cast(StateDict, state)
        state_messages = cast(List[BaseMessage], state_dict.get("messages") or [])
        retrieval_state = cast(dict[str, Any], state_dict.get("retrieval") or {})
        history_messages = select_conversation_history(
            state_messages, limit=RESPONSE_HISTORY_LIMIT
        )
        summary_enabled = bool(state_dict.get("conversation_summary_enabled", True))
        conversation_summary = (
            state_dict.get("conversation_summary") or "" if summary_enabled else ""
        )
        rewrite_msg = cast(
            Optional[BaseMessage], retrieval_state.get("rewritten_query_message")
        )
        rewritten_query = ""
        if rewrite_msg is not None:
            msg_content = getattr(rewrite_msg, "content", "") or ""
            if isinstance(msg_content, str):
                rewritten_query = msg_content
            else:
                rewritten_query = message_to_text(rewrite_msg)
        if not rewritten_query:
            summary_query = state_dict.get("summary_search_query")
            if isinstance(summary_query, BaseMessage):
                msg_content = getattr(summary_query, "content", "") or ""
                if isinstance(msg_content, str):
                    rewritten_query = msg_content
                else:
                    rewritten_query = message_to_text(summary_query)
            elif isinstance(summary_query, str):
                rewritten_query = summary_query
        gen_inputs = generation_inputs_from_state(state_dict)
        task_type = gen_inputs["task_type"]
        intent = gen_inputs["intent"]
        user_language = gen_inputs["user_language"]
        normalized_question = gen_inputs["normalized_question"]
        followup_instruction = gen_inputs["followup_instruction"]
        prev_answer = gen_inputs["prev_answer"]
        context_text = gen_inputs["context_text"]
        used_tools = gen_inputs["used_tools"]
        loop_count = gen_inputs["loop_count"]
        is_followup = gen_inputs["is_followup"]
        is_out_of_scope = gen_inputs["is_out_of_scope"]

        emit_node_event(
            writer,
            node="response_synth",
            phase="generation",
            payload={
                "channel": "status",
                "stage": AskStreamStages.RESPONSE_GENERATING,
                "intent": intent,
                "used_tools": used_tools,
                "loop": loop_count,
                "is_out_of_scope": is_out_of_scope,
                "followup": is_followup,
            },
        )

        # ä½¿ç”¨çµ±ä¸€çš„ system prompt å»ºæ§‹å‡½æ•¸
        complete_system_prompt = _build_complete_system_prompt(
            is_followup=is_followup,
            user_language=user_language,
            prompt_service=prompt_service,
            # Followup å°ˆç”¨
            prev_answer=prev_answer,
            # Non-followup å°ˆç”¨
            task_type=task_type,
            intent=intent,
            used_tools=used_tools,
            context_text=context_text,
            # å…±ç”¨
            conversation_summary=conversation_summary,
        )

        final_messages: List[BaseMessage] = [SystemMessage(content=complete_system_prompt)]
        if history_messages:
            final_messages.extend(history_messages)
        final_messages.append(
            HumanMessage(content=followup_instruction if is_followup else normalized_question)
        )

        # å‹•æ…‹å»ºç«‹ LLMï¼Œæ ¹æ“š task_type èª¿æ•´åƒæ•¸
        streaming_llm = _create_streaming_llm(task_type)

        answer_tokens: List[str] = []
        reasoning_tokens: List[str] = []
        final_usage: Optional[dict[str, Any]] = None
        response_id: Optional[str] = None
        reasoning_started = False
        first_token_at: Optional[float] = None
        is_fallback = False
        fallback_error: Optional[Exception] = None

        try:
            async with llm_concurrency.acquire(agent_backend):
                async for chunk in streaming_llm.astream(final_messages):
                    now = time.monotonic()
                    if first_token_at is None:
                        first_token_at = now

                    add_kwargs = getattr(chunk, "additional_kwargs", {}) or {}
                    channel = add_kwargs.get("channel")
                    delta = add_kwargs.get("delta") or ""
                    content = getattr(chunk, "content", None)
                    if isinstance(content, str) and content and not delta:
                        delta = content

                    if channel in ("reasoning", "reasoning_summary"):
                        if delta:
                            reasoning_tokens.append(delta)
                            if not reasoning_started:
                                reasoning_started = True
                                emit_node_event(
                                    writer,
                                    node="response_synth",
                                    phase="generation",
                                    payload={
                                        "channel": "status",
                                        "stage": AskStreamStages.RESPONSE_REASONING,
                                    },
                                )
                            writer(
                                {
                                    "source": "response_synth",
                                    "node": "response_synth",
                                    "phase": "generation",
                                    "channel": "reasoning",
                                    "delta": delta,
                                }
                            )
                    elif channel == "meta":
                        responses_meta = add_kwargs.get("responses_meta")
                        if isinstance(responses_meta, dict):
                            usage_dict = normalize_usage_payload(responses_meta.get("usage"))
                            if usage_dict:
                                final_usage = usage_dict
                            resp_id = responses_meta.get("response_id") or responses_meta.get("id")
                            if resp_id and not response_id:
                                response_id = str(resp_id)
                    elif delta and channel not in ("done",):
                        answer_tokens.append(delta)
                        writer(
                            {
                                "source": "response_synth",
                                "node": "response_synth",
                                "phase": "generation",
                                "channel": "answer",
                                "delta": delta,
                            }
                        )

                    usage_from_chunk = normalize_usage_payload(getattr(chunk, "usage_metadata", None))
                    if usage_from_chunk:
                        final_usage = usage_from_chunk
                    else:
                        response_metadata = getattr(chunk, "response_metadata", None)
                        if isinstance(response_metadata, dict):
                            usage_from_response_meta = normalize_usage_payload(
                                response_metadata.get("token_usage")
                            )
                            if usage_from_response_meta:
                                final_usage = usage_from_response_meta

        except Exception as stream_error:  # noqa: BLE001 pylint: disable=broad-exception-caught
            # LLM streaming å¤±æ•—ï¼Œä½¿ç”¨ fallback å›æ‡‰
            is_fallback = True
            fallback_error = stream_error
            logger.error(
                "[RESPONSE] LLM streaming failed, using fallback: %s",
                stream_error,
            )

            # ç”¢ç”Ÿ fallback å›æ‡‰
            from chatbot_rag.core.config import settings
            fallback_answer = _generate_fallback_response(
                intent=intent,
                user_language=user_language,
                error=stream_error,
                include_error_hint=settings.debug,
            )

            # ç™¼é€ fallback äº‹ä»¶
            emit_node_event(
                writer,
                node="response_synth",
                phase="generation",
                payload={
                    "channel": "status",
                    "stage": AskStreamStages.RESPONSE_FALLBACK,
                    "error": str(stream_error),
                    "error_type": type(stream_error).__name__,
                },
            )

            # å¯«å…¥ fallback å›æ‡‰
            writer(
                {
                    "source": "response_synth",
                    "node": "response_synth",
                    "phase": "generation",
                    "channel": "answer",
                    "delta": fallback_answer,
                }
            )
            answer_tokens = [fallback_answer]

        final_answer = "".join(answer_tokens)
        latest_question = state_dict.get("latest_question") or normalized_question
        if summary_enabled:
            (
                updated_summary,
                summary_usage,
                _,
            ) = await _summarize_conversation_history(
                prev_summary=conversation_summary,
                latest_user=latest_question,
                latest_answer=final_answer,
                base_llm_params=base_llm_params,
                prompt_service=prompt_service,
            )
        else:
            updated_summary = ""
            summary_usage = None

        meta_payload: Dict[str, Any] = {
            "response_id": response_id,
            "usage": final_usage,
            "loops": loop_count,
            "used_tools": used_tools,
            "eval_query_rewrite": rewritten_query,
            "channels": {
                "output_text": {
                    "text": final_answer,
                    "char_count": len(final_answer),
                }
            },
        }
        if is_fallback:
            meta_payload["is_fallback"] = True
            meta_payload["fallback_error_type"] = (
                type(fallback_error).__name__ if fallback_error else "unknown"
            )
        if summary_enabled:
            meta_payload["conversation_summary"] = updated_summary

        writer(
            {
                "source": "response_synth",
                "node": "response_synth",
                "phase": "generation",
                "channel": "meta",
                "meta": meta_payload,
            }
        )

        emit_node_event(
            writer,
            node="response_synth",
            phase="generation",
            payload={
                "channel": "status",
                "stage": AskStreamStages.RESPONSE_DONE,
                "intent": intent,
                "loops": loop_count,
                "used_tools": used_tools,
                "is_out_of_scope": is_out_of_scope,
            },
        )

        final_msg = AIMessage(
            content=final_answer,
            additional_kwargs={
                "reasoning_text": "".join(reasoning_tokens),
                "responses_meta": meta_payload,
            },
        )

        new_state = cast(State, dict(state))
        new_state["messages"] = state_dict.get("messages", []) + [final_msg]
        new_state["final_answer"] = final_answer
        new_state["response_meta"] = meta_payload
        new_state["intent"] = intent
        new_state["is_out_of_scope"] = is_out_of_scope
        new_state["eval_question"] = latest_question
        new_state["eval_context"] = context_text
        new_state["eval_answer"] = final_answer
        cast(StateDict, new_state)["eval_query_rewrite"] = rewritten_query
        cast(StateDict, new_state)["conversation_summary"] = (
            updated_summary if summary_enabled else ""
        )
        cast(StateDict, new_state)["conversation_summary_enabled"] = summary_enabled
        if summary_enabled and summary_usage:
            emit_llm_meta_event(
                writer,
                node="response_synth",
                phase="generation",
                component="conversation_summary",
                usage=summary_usage,
            )
        return new_state

    return response_node
